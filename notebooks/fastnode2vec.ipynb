{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "#from scipy.spatial.distance import cdist\n",
    "from nltk import bigrams\n",
    "import networkx as nx\n",
    "#similarity score concat before fc1 \n",
    "import tldextract\n",
    "from itertools import combinations\n",
    "from shutil import which\n",
    "import torch.nn.functional as F\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from cleantext import clean\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import preprocessor as p \n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import LongformerModel, LongformerTokenizer, LongformerConfig\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"my bar!\")\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "#import wandb\n",
    "#import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "import gensim.models as gsm\n",
    "\n",
    "\n",
    "\n",
    "import emoji #pip install emoji --upgrade\n",
    "# Internal dependencies\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1, 2, 3' \n",
    "#....to install apex...\n",
    "#pip install -v --no-cache-dir ./\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "#from torchnlp.nn import Attention #pip imstall pytorch-nlp\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "#from fastnode2vec import Graph, Node2Vec\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from cleantext import clean\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import preprocessor as p \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import LongformerModel, LongformerTokenizer, LongformerConfig\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"my bar!\")\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "#import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex \n",
    "#pip install emoji --upgrade\n",
    "# Internal dependencies\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributed as dist\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "from fastnode2vec import Graph, Node2Vec\n",
    "import tldextract\n",
    "import networkx as nx\n",
    "from nltk import bigrams\n",
    "def makefolder(save):\n",
    "    if  os.path.exists(save) == False:\n",
    "        os.makedirs(save)\n",
    "    else:\n",
    "        print('Floder:',save,'Exsit!')\n",
    "    return save\n",
    "def extract_domain(x):\n",
    "    '''\n",
    "    e.g. x='http://www.bbc.co.uk'\n",
    "    '''\n",
    "    ext = tldextract.extract(x)\n",
    "    return ext.registered_domain\n",
    "def updatecol(df1,df2,colname,index):\n",
    "    df1[colname]=''\n",
    "\n",
    "    df1.set_index(index,inplace=True)\n",
    "\n",
    "    df2.set_index(index,inplace=True)\n",
    "\n",
    "    df1.update(df2)\n",
    "\n",
    "    df1.reset_index(inplace=True)\n",
    "    df2.reset_index(inplace=True)\n",
    "    return df1\n",
    "def fast_node2vec(_df):\n",
    "                '''\n",
    "                _df:retweetdf,mentiondf\n",
    "                _save:saving folder path\n",
    "                _file:csv name, e.g., vaccine, mask\n",
    "                saving_name: RT or MT\n",
    "                '''\n",
    "\n",
    "                _tuples=_df.to_records(index=False)\n",
    "\n",
    "                _lst=list(_tuples)\n",
    "\n",
    "                _graph = Graph(_lst, directed=False, weighted=False)\n",
    "\n",
    "                _n2v = Node2Vec(_graph, dim=300, walk_length=500, context=10, p=2.0, q=0.5, workers=-1)\n",
    "\n",
    "                _n2v.train(epochs=1000)\n",
    "\n",
    "\n",
    "                return _n2v\n",
    "def bihashtagGraph(_lst,_i):\n",
    "    _i=0\n",
    "    #terms_bigram = [list(bigrams(tweet)) if len(tweet)>1 else (tweet,'') for tweet in _lst]\n",
    "    terms_bigram = [list(bigrams(tweet)) for tweet in _lst]\n",
    "\n",
    "    d=[x for x in terms_bigram if x!=[]]\n",
    "\n",
    "    lst=[]\n",
    "    for each in d:\n",
    "        if len(each)==0:\n",
    "            lst.append(each)\n",
    "        else:\n",
    "            [lst.append(x) for x in each]\n",
    "\n",
    "    _hadf=pd.DataFrame(lst)\n",
    "\n",
    "    _K=nx.from_pandas_edgelist(_hadf,0,1)\n",
    "\n",
    "    nx.write_gexf(_K,'../Project2/'+topic+'/'+str(_i)+'_cooccur.gexf')\n",
    "    return _K,_hadf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def export_tSNE(_data,save):\n",
    "    _array = _data.iloc[:,0:300].to_numpy()\n",
    "    time_start = time.time()\n",
    "    tsne = TSNE(n_components=2, verbose=1, random_state=42)\n",
    "    z = tsne.fit_transform(_array) \n",
    "    print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"y\"] = _data.labels\n",
    "    df[\"comp-1\"] = z[:,0]\n",
    "    df[\"comp-2\"] = z[:,1]\n",
    "\n",
    "    sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),data=df).set(title=_target.capitalize()+\" tSNE projection\") \n",
    "    plt.savefig('../Project2/'+topic+'/figs/tSNE_'+_target+'_'+save+'_drop_duplicates.pdf')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for topic in ['Ivermectin']:\n",
    "    file_name=topic\n",
    "    path = '../Project2/'+topic+'/'\n",
    "    ranking='random'\n",
    "    \n",
    "    batch_size = 16\n",
    "    which_time = 0\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Training')\n",
    "    print('Device:',device)\n",
    "    N_EPOCHS = 100\n",
    "    HIDDEN_DIM = 300 \n",
    "    OUTPUT_DIM = 5\n",
    "    N_LAYERS = 5\n",
    "    DROPOUT = 0.0\n",
    "\n",
    "    for _target in ['domain']:\n",
    "        \n",
    "\n",
    "        if topic=='biden':\n",
    "                ndf=pd.read_csv(path+topic+'_selected.csv')\n",
    "                print(ndf.shape)\n",
    "                ndf.dropna(subset=['retweeted_status_screen_name'],inplace=True)\n",
    "                print(ndf.shape)\n",
    "                m2=52\n",
    "                label1='Democratic'\n",
    "                label2='Republican'\n",
    "\n",
    "        elif topic=='Ivermectin':\n",
    "                ndf=pd.read_csv(path+topic+'_selected_noNA.csv')\n",
    "\n",
    "                m2=241\n",
    "                label1='Misinformation-related'\n",
    "                label2='Mainstream'\n",
    "                \n",
    "        \n",
    "            \n",
    "            \n",
    "        ndf['urls']=ndf['urls'].str.split()\n",
    "        ndf=ndf.explode('urls')\n",
    "        ndf['domain']=ndf['urls'].apply(lambda x:  extract_domain(x))\n",
    "        modularity=pd.read_csv(path+file_name+'_networkx.csv')\n",
    "\n",
    "        modularity=modularity.rename(columns={'Id':'user_screen_name'})\n",
    "        ndf=updatecol(ndf,modularity,'modularity_class','user_screen_name')\n",
    "\n",
    "\n",
    "        nodes1=modularity[modularity['modularity_class']==38]['user_screen_name'].drop_duplicates().tolist()# Iverlectin related\n",
    "        nodes2=modularity[modularity['modularity_class']==m2]['user_screen_name'].drop_duplicates().tolist()# 241 mainstream related\n",
    "\n",
    "\n",
    "        print('Processing finished!')\n",
    "\n",
    "\n",
    "\n",
    "        ndf1=ndf.groupby(['user_screen_name'])[_target].apply(list).reset_index()\n",
    "        lst=ndf1[_target].apply(list).tolist()\n",
    "        G,hdf=bihashtagGraph(lst,topic+'_'+_target)\n",
    "\n",
    "\n",
    "\n",
    "        print(len(G.nodes()))\n",
    "\n",
    "        model=fast_node2vec(hdf)\n",
    "\n",
    "        print(len(model.wv.index2entity))\n",
    "\n",
    "        ndf1=ndf.groupby(['user_screen_name'])[_target].apply(list).reset_index()\n",
    "\n",
    "        import gensim\n",
    "        def convert_vec(_model,x):\n",
    "            try:\n",
    "                if len(x)==0:\n",
    "                    if int(gensim.__version__[0])<4:\n",
    "\n",
    "                        return _model.wv.get_vector(x[0])\n",
    "                    else:\n",
    "                        return _model.get_vector(x[0])\n",
    "\n",
    "                else:\n",
    "                    if int(gensim.__version__[0])<4:\n",
    "                        _lst= [_model.wv.get_vector(y) for y in x]\n",
    "                    else:\n",
    "                         _lst= [_model.get_vector(y) for y in x]\n",
    "\n",
    "                    _sum=0\n",
    "                    for each in _lst:\n",
    "                        _sum=_sum+each\n",
    "                    return _sum\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "        if _target=='domain':\n",
    "            ndf=ndf.explode(['domain'])\n",
    "            #ndf[_target]=ndf[_target].apply(lambda x: ','.join(x))\n",
    "\n",
    "            ndf1=ndf.groupby(by='user_screen_name').agg(domain=('domain', lambda x: \",\".join(set(x)))).reset_index()\n",
    "\n",
    "        elif _target=='hashtag':\n",
    "\n",
    "\n",
    "            ndf1=ndf.groupby(by='user_screen_name').agg(hashtags=(_target, lambda x: \",\".join(set(x)))).reset_index()\n",
    "\n",
    "        #ndf1[_target]=ndf1[_target].str.split(',')\n",
    "\n",
    "\n",
    "\n",
    "        ndf1[_target]=ndf1[_target].str.split(',')\n",
    "\n",
    "        ndf1['vec']=ndf1[_target].apply(lambda x: convert_vec(model,x))\n",
    "\n",
    "        #ndf1=ndf1.rename(columns={_target:'vec'})\n",
    "\n",
    "        ndf1.dropna(subset=['vec'], inplace=True)\n",
    "\n",
    "        ndf1.reset_index(inplace=True,drop=True)\n",
    "\n",
    "        dfv=ndf1['vec']\n",
    "        dfv=pd.DataFrame(dfv)\n",
    "\n",
    "        values=np.array([x[0] for x in dfv.values])\n",
    "\n",
    "        df_values=pd.DataFrame(values)\n",
    "\n",
    "        df_values['user_screen_name']=ndf1['user_screen_name']\n",
    "\n",
    "        data=df_values\n",
    "\n",
    "        data.drop_duplicates(subset=list(range(300)),inplace=True)\n",
    "        \n",
    "\n",
    "        data.loc[data['user_screen_name'].isin(nodes1),'labels']=label1\n",
    "\n",
    "        data.loc[data['user_screen_name'].isin(nodes2),'labels']=label2\n",
    "        \n",
    "        data.dropna(subset=['labels'],inplace=True)\n",
    "\n",
    "        smaller_sample=500\n",
    "        Nums=[smaller_sample, \n",
    "              data.groupby('labels').size().reset_index().loc[0,0],\n",
    "              data.groupby('labels').size().reset_index().loc[1,0]]\n",
    "        num = min(Nums)\n",
    "        savingname=topic+'_'+_target+'_'+str(num*2)\n",
    "        print('Num of samples:',num)\n",
    "\n",
    "        ndf_c=data[data.labels==label1]\n",
    "\n",
    "        ndf_n=data[data.labels==label2]\n",
    "\n",
    "        ndf_c=ndf_c.sample(num)\n",
    "\n",
    "        ndf_n=ndf_n.sample(num)\n",
    "\n",
    "        df=pd.concat([ndf_c,ndf_n])\n",
    "\n",
    "        export_tSNE(df,str(num)+topic+ranking)\n",
    "\n",
    "    df.loc[df['labels']==label1, 'labels']=1\n",
    "\n",
    "    df.loc[df['labels']==label2, 'labels']=0\n",
    "    df['labels']=df['labels'].apply('int32')\n",
    "\n",
    "    def split_data(_df):\n",
    "        np.random.seed(42)\n",
    "        msk = np.random.rand(len(_df)) < 0.8\n",
    "        df_train = _df[msk]\n",
    "        df_left = _df[~msk]\n",
    "        np.random.seed(42)\n",
    "        msk = np.random.rand(len(df_left)) < 0.5\n",
    "        df_valid = df_left[msk]\n",
    "        df_test = df_left[~msk]\n",
    "\n",
    "        print(df.shape)\n",
    "        print('train:',df_train.shape)\n",
    "        print('validation',df_valid.shape)\n",
    "        print('test',df_test.shape)\n",
    "\n",
    "        # Get the lists of sentences and their labels.\n",
    "\n",
    "        training_data = torch.from_numpy(df_train.iloc[:,0:300].to_numpy())\n",
    "        validation_data = torch.from_numpy(df_valid.iloc[:,0:300].to_numpy())\n",
    "        testing_data = torch.from_numpy(df_test.iloc[:,0:300].to_numpy())\n",
    "\n",
    "        labels_train = torch.tensor(df_train['labels'].to_numpy())\n",
    "        labels_valid = torch.tensor(df_valid['labels'].to_numpy())\n",
    "        labels_test = torch.tensor(df_test['labels'].to_numpy()) \n",
    "\n",
    "\n",
    "        _train_dataset = TensorDataset(training_data, labels_train)\n",
    "        _val_dataset =   TensorDataset(validation_data, labels_valid)\n",
    "        _test_dataset =  TensorDataset(testing_data, labels_test)\n",
    "\n",
    "\n",
    "        return _train_dataset, _val_dataset, _test_dataset\n",
    "    train_dataset, val_dataset, test_dataset= split_data(df)\n",
    "\n",
    "    import wandb\n",
    "    class NetworkMLP(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(NetworkMLP, self).__init__() \n",
    "            self.fc1 = nn.Linear(300, 150)\n",
    "\n",
    "        def forward(self, X):\n",
    "            z1 = self.fc1(X)\n",
    "            return z1 \n",
    "\n",
    "    class JointModel(nn.Module):\n",
    "\n",
    "        def __init__(self,hidden_dim, output_dim, n_layers, dropout):\n",
    "            super().__init__()\n",
    "            self.model_net = NetworkMLP()\n",
    "            self.fc1 = nn.Linear(150*1, 600)\n",
    "            self.fc2 = nn.Linear(600, output_dim) #original\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "        def forward(self, _net): \n",
    "            prediction= self.model_net(_net)\n",
    "\n",
    "            #prediction_twtNews = self.model_twt(x_tNews)\n",
    "\n",
    "            #concat_pred = torch.cat((prediction_RT, prediction_MT), 1)\n",
    "            output = self.fc1(prediction)\n",
    "\n",
    "            output = self.dropout(output) # add dropout\n",
    "            output = self.fc2(F.relu(output)) #original\n",
    "            #output = self.dropout(output)#add dropout\n",
    "            #output = self.fc3(F.relu(output))\n",
    "            return output\n",
    "\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    def binary_accuracy(preds, y):\n",
    "        \"\"\"\n",
    "        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "        \"\"\"\n",
    "\n",
    "        #round predictions to the closest integer\n",
    "        #rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        #rounded_preds = torch.round(preds)\n",
    "        correct = (preds == y).float() #convert into float for division \n",
    "        acc = correct.sum() / len(correct)\n",
    "\n",
    "        from sklearn.metrics import f1_score\n",
    "        macro_f1 = f1_score(y.to(\"cpu\"), preds.to(\"cpu\"), average='macro')\n",
    "        #print(\"macro_f1\", macro_f1)\n",
    "\n",
    "        return acc, macro_f1\n",
    "\n",
    "\n",
    "    def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        epoch_macro = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(iterator):    \n",
    "        #for batch in iterator:\n",
    "            #print(\"batch\", batch)\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            #   b_input_ids = batch[0].to(device)\n",
    "            #   b_input_mask = batch[1].to(device)\n",
    "            #   b_labels = batch[2].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            net = batch[0].to(device)\n",
    "            #netMT = batch[1].to(device)\n",
    "            #feature1=batch[0]\n",
    "            #print('feature1:',feature1)\n",
    "            #feature2=batch[1]\n",
    "            #cosine_score=emb_similarity(feature1, feature2)\n",
    "            #cosine_score_view=cosine_score.view(cosine_score.size()[0],1)\n",
    "\n",
    "            #twt = batch[0].to(device)\n",
    "            #news= batch[1].to(device)\n",
    "\n",
    "            #print('text.size:',twt.size())\n",
    "            #print('news.size:',news.size())\n",
    "\n",
    "            #cosine_similarity=cosine_similarity.to(device)\n",
    "            label = batch[1].to(device)\n",
    "\n",
    "            #print(\"label\", label, type(label),label.size()) #torch.Size([32])\n",
    "            #label = label.unsqueeze(1)\n",
    "            #print(\"label\", label, type(label),label.size())\n",
    "            #predictions = model(batch.text).squeeze(1)\n",
    "            #predictions = model(text)\n",
    "            predictions = model(net)#,cosine_similarity)\n",
    "\n",
    "            #cosine_score_view=cosine_score.view(cosine_score.size()[0],1)\n",
    "\n",
    "\n",
    "\n",
    "            #print('cosine_similarity.size:',cosine_score.size())\n",
    "            #predictions=torch.cat((predictions,cosine_score_view), dim = 1)\n",
    "            #print('predictions.size:',predictions.size())\n",
    "            label = label.type(torch.LongTensor)\n",
    "\n",
    "\n",
    "            loss = criterion(predictions, label)\n",
    "\n",
    "            acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_macro += macro_f1.item()\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)\n",
    "\n",
    "    def evaluate(model, iterator, criterion):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        epoch_macro = 0\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            #for batch in iterator:\n",
    "            for step, batch in enumerate(iterator):    \n",
    "                #des = batch[0].to(device)\n",
    "                #loc = batch[1].to(device)\n",
    "                net = batch[0].to(device)\n",
    "                #netMT = batch[1].to(device)\n",
    "                #twt = batch[0].to(device)\n",
    "                #news= batch[1].to(devifce)\n",
    "                label = batch[1].to(device)\n",
    "                #predictions = model(batch.text).squeeze(1)\n",
    "                label = label.type(torch.LongTensor)\n",
    "                predictions = model(net)\n",
    "                #loss = criterion(predictions, batch.label)\n",
    "                loss = criterion(predictions, label)\n",
    "                #acc = binary_accuracy(predictions, batch.label)\n",
    "                acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "                epoch_macro += macro_f1.item()\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)\n",
    "\n",
    "\n",
    "    def epoch_time(start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs\n",
    "\n",
    "    def save_plots(train_losses, val_losses, train_f1, val_f1, train_accs, val_accs,test_accs,test_f1s):\n",
    "        \"\"\"Plot\n",
    "\n",
    "            Plot two figures: loss vs. epoch and accuracy vs. epoch\n",
    "        \"\"\"\n",
    "        n = len(train_losses)\n",
    "        xs = np.arange(1,n+1,1)\n",
    "        xs = xs.astype(int)\n",
    "\n",
    "        # plot train and val losses\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(xs, train_losses, '--', linewidth=2, label='train loss')\n",
    "        ax.plot(xs, val_losses, '-', linewidth=2, label='validation loss')\n",
    "        #ax.set_xlim(0, 10)\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend(loc='upper right')\n",
    "        plt.savefig('../Project2/'+topic+'/figs/'+savingname+'_'+str(DROPOUT)+'_Loss.png')\n",
    "\n",
    "        # plot train and val f1-score\n",
    "        #plt.clf()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(xs, train_f1, '--', linewidth=2, label='train')\n",
    "        ax.plot(xs, val_f1, '-', linewidth=2, label='validation')\n",
    "        ax.plot(xs, test_f1s, '-', linewidth=2, label='test')\n",
    "\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Macro-avg F1\")\n",
    "        ax.legend(loc='lower right')\n",
    "        plt.savefig('../Project2/'+topic+'/figs/'+savingname+'_'+str(DROPOUT)+'_Macro-avgF1.png')\n",
    "\n",
    "        # plot train and val accuracy\n",
    "        #plt.clf()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(xs, train_accs, '--', linewidth=2, label='train')\n",
    "        ax.plot(xs, val_accs, '-', linewidth=2, label='validation')\n",
    "        ax.plot(xs, test_accs, '-', linewidth=2, label='test')\n",
    "        #ax.set_xlim(0, 10)\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Accuracy\")\n",
    "        ax.legend(loc='lower right')\n",
    "        plt.savefig('../Project2/'+topic+'/figs/'+savingname+'_'+str(DROPOUT)+'_ACC.png')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "                        train_dataset,  # The training samples.\n",
    "                        sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "                        batch_size = batch_size # Trains with this batch size.\n",
    "                    )\n",
    "    validation_dataloader = DataLoader(\n",
    "                val_dataset, # The validation samples.\n",
    "                sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "                batch_size = batch_size # Evaluate with this batch size.\n",
    "            )\n",
    "    test_dataloader = DataLoader(\n",
    "                test_dataset, # The validation samples.\n",
    "                sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "                batch_size = batch_size # Evaluate with this batch size.\n",
    "            ) \n",
    "\n",
    "    model = JointModel(HIDDEN_DIM, OUTPUT_DIM, N_LAYERS,DROPOUT)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(device) \n",
    "    wandb.login()\n",
    "    wandb.init(project='Project2_joint',name=savingname)#wandb.init(project='Project2_joint',name=savingname+'_'+str(eachtime))\n",
    "    wandb.config = {\n",
    "            \"epochs\": N_EPOCHS,\n",
    "            \"batch_size\": batch_size\n",
    "            }\n",
    "\n",
    "    resume_training = False\n",
    "    if resume_training == True:\n",
    "        eachtime = 0\n",
    "        which_time = 6\n",
    "        path= '../Project2/pt/'+savingname+'_'+str(eachtime)+'/'\n",
    "        checkpoint = torch.load(path+str(which_time)+'.pt')\n",
    "        model.load_state_dict(checkpoint)\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pt=makefolder('../Project2/pt/'+savingname+'/')#pt=makefolder('../Project2/pt/'+savingname+'_'+str(eachtime)+'/')\n",
    "    pt_best=makefolder('../Project2/pt_best/'+savingname+'/')#pt_best=makefolder('../Project2/pt_best/'+savingname+'_'+str(eachtime)+'/')\n",
    "\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    per_epoch_train_loss = []\n",
    "    per_epoch_val_loss = []\n",
    "    per_epoch_train_f1 = []\n",
    "    per_epoch_val_f1 = []\n",
    "    per_epoch_train_acc = []\n",
    "    per_epoch_val_acc = []\n",
    "    import time\n",
    "    t1=time.time()\n",
    "    for epoch in range(which_time, N_EPOCHS):\n",
    "        print(epoch)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc, train_f1 = train(model, train_dataloader, optimizer, criterion)\n",
    "        valid_loss, valid_acc, valid_f1 = evaluate(model, validation_dataloader, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        per_epoch_train_loss.append(train_loss)\n",
    "        per_epoch_val_loss.append(valid_loss)\n",
    "        per_epoch_train_f1.append(train_f1)\n",
    "        per_epoch_val_f1.append(valid_f1)\n",
    "        per_epoch_train_acc.append(train_acc)\n",
    "        per_epoch_val_acc.append(valid_acc)\n",
    "        Val_aver_acc=mean(per_epoch_val_acc)\n",
    "        Val_aver_f1=mean(per_epoch_val_f1)\n",
    "\n",
    "\n",
    "        if valid_loss <= best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            print(\"best model saved in epoch :\", epoch+1 )\n",
    "            torch.save(model.state_dict(), pt_best +str(epoch+1)+'.pt')\n",
    "        torch.save(model.state_dict(), pt +'/'+str(epoch+1)+'.pt')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} | Train macro-avg-f1: {train_f1*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f} |  Val. macro-avg-f1: {valid_f1*100:.2f}%')\n",
    "        print(f'\\t Aver. ACC: {Val_aver_acc*100:.2f}%')\n",
    "        wandb.log({'Max Val Acc':max(per_epoch_val_acc),'Average Val. Acc': Val_aver_acc,'Val. accuracy': valid_acc, 'Val. loss': valid_loss,'Val. macro-avg-f1':valid_f1})\n",
    "\n",
    "\n",
    "    Test_Acc = []\n",
    "    Test_f1=[]\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        model.load_state_dict(torch.load(pt+str(epoch+1)+'.pt')) \n",
    "        test_loss, test_acc, test_f1 = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "        Test_Acc.append(test_acc)\n",
    "        Test_f1.append(test_f1)\n",
    "        Test_aver_acc=mean(Test_Acc)\n",
    "        Test_test_f1=mean(Test_f1)\n",
    "        t2=time.time()\n",
    "        print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f} | Test macro-avg-f1: {test_f1*100:.2f}%')\n",
    "        print(f'\\t Aver. TEST ACC: {Test_aver_acc*100:.2f}%')\n",
    "        wandb.log({'Max TEST Acc':max(Test_Acc),'Average TEST Acc': Test_aver_acc,'Test accuracy': test_acc, 'Test loss': test_loss,'Test macro-avg-f1':Test_test_f1})\n",
    "    _time=t2-t1\n",
    "    wandb.finish()\n",
    "    save_plots(per_epoch_train_loss, per_epoch_val_loss, per_epoch_train_f1, \n",
    "    per_epoch_val_f1, per_epoch_train_acc, per_epoch_val_acc,Test_Acc,Test_f1)\n",
    "    re=pd.DataFrame([[topic,N_LAYERS,DROPOUT, N_EPOCHS,OUTPUT_DIM, _target,num*2,Val_aver_acc,Test_aver_acc, Val_aver_f1, Test_test_f1,_time]],columns=['Topic','Layer','Dropout','Epochs','Output_dim','NEWS/TEXT','samples','Avg. Val. Acc.','Avg. Test.Acc','Avg. Val. macro-avg-f1','Avg. Test. macro-avg-f1','Time'])\n",
    "    re.to_csv('../Project2/DOMAIN_HASHTAG_classification_results_drop_duplicates.csv',index=None,mode='a')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
