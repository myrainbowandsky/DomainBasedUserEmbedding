{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pandas as pd\\nimport networkx as nx\\n\\npath='../Project2/health/'\\n\\ndf=pd.read_csv(path+'health.csv')\\n\\ndf=df[df['lang']=='en']\\nndf=df[['user_screen_name','retweeted_status_screen_name']]\\n\\nndf.dropna(subset=['retweeted_status_screen_name'],inplace=True)\\n\\n\\n\\nG=nx.from_pandas_edgelist(ndf,'user_screen_name','retweeted_status_screen_name',create_using=nx.DiGraph())\\n\\nnx.write_gexf(G,path+'health_en.gexf')\\n\\nmdf=df[['created_at','user_screen_name','retweeted_status_screen_name','text','urls','hashtags']]\\n\\ndf.columns\\n\\nmdf.to_csv(path+'health_selected_en.csv',index=None)\\n\\nmdf.dropna(subset=['retweeted_status_screen_name'],inplace=True)\\n\\nmdf.to_csv(path+'health_selected_noNA_en.csv',index=None)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "path='../Project2/health/'\n",
    "\n",
    "df=pd.read_csv(path+'health.csv')\n",
    "\n",
    "df=df[df['lang']=='en']\n",
    "ndf=df[['user_screen_name','retweeted_status_screen_name']]\n",
    "\n",
    "ndf.dropna(subset=['retweeted_status_screen_name'],inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "G=nx.from_pandas_edgelist(ndf,'user_screen_name','retweeted_status_screen_name',create_using=nx.DiGraph())\n",
    "\n",
    "nx.write_gexf(G,path+'health_en.gexf')\n",
    "\n",
    "mdf=df[['created_at','user_screen_name','retweeted_status_screen_name','text','urls','hashtags']]\n",
    "\n",
    "df.columns\n",
    "\n",
    "mdf.to_csv(path+'health_selected_en.csv',index=None)\n",
    "\n",
    "mdf.dropna(subset=['retweeted_status_screen_name'],inplace=True)\n",
    "\n",
    "mdf.to_csv(path+'health_selected_noNA_en.csv',index=None)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Health topic uses English tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pandas as pd\\nimport os\\nimport networkx as nx\\n\\npath='../Project2/health/3in1/'\\n\\nfiles=os.listdir(path)\\n\\nfor each in files:\\n    each_file = path+each\\n    \\n    each_name=each.split('.')[0]\\n\\n    df=pd.read_csv(path + each)\\n\\n    df=df[df['lang']=='en']\\n    ndf=df[['user_screen_name','retweeted_status_screen_name']]\\n\\n    ndf.dropna(subset=['retweeted_status_screen_name'],inplace=True)\\n\\n    G=nx.from_pandas_edgelist(ndf,'user_screen_name','retweeted_status_screen_name',create_using=nx.DiGraph())\\n\\n    nx.write_gexf(G,path+each_name+'.gexf')\\n\\n    mdf=df[['created_at','user_screen_name','retweeted_status_screen_name','text','urls','hashtags']]\\n\\n\\n    mdf.to_csv(path+'health_selected'+each_name+'.csv',index=None)\\n\\n    mdf.dropna(subset=['retweeted_status_screen_name'],inplace=True)\\n\\n    mdf.to_csv(path+each_name+'_selected_noNA.csv',index=None)\\n    \\n    print(mdf.shape)\\n    \\n    #url=mdf.drop_duplicates(subset=['urls'])\\n    #url=url[['urls']]\\n    #url.to_csv('../Project2/health/3in1/'+each_name+'urls.csv',index=None)\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use English tweets\n",
    "'''import pandas as pd\n",
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "path='../Project2/health/3in1/'\n",
    "\n",
    "files=os.listdir(path)\n",
    "\n",
    "for each in files:\n",
    "    each_file = path+each\n",
    "    \n",
    "    each_name=each.split('.')[0]\n",
    "\n",
    "    df=pd.read_csv(path + each)\n",
    "\n",
    "    df=df[df['lang']=='en']\n",
    "    ndf=df[['user_screen_name','retweeted_status_screen_name']]\n",
    "\n",
    "    ndf.dropna(subset=['retweeted_status_screen_name'],inplace=True)\n",
    "\n",
    "    G=nx.from_pandas_edgelist(ndf,'user_screen_name','retweeted_status_screen_name',create_using=nx.DiGraph())\n",
    "\n",
    "    nx.write_gexf(G,path+each_name+'.gexf')\n",
    "\n",
    "    mdf=df[['created_at','user_screen_name','retweeted_status_screen_name','text','urls','hashtags']]\n",
    "\n",
    "\n",
    "    mdf.to_csv(path+'health_selected'+each_name+'.csv',index=None)\n",
    "\n",
    "    mdf.dropna(subset=['retweeted_status_screen_name'],inplace=True)\n",
    "\n",
    "    mdf.to_csv(path+each_name+'_selected_noNA.csv',index=None)\n",
    "    \n",
    "    print(mdf.shape)\n",
    "    \n",
    "    #url=mdf.drop_duplicates(subset=['urls'])\n",
    "    #url=url[['urls']]\n",
    "    #url.to_csv('../Project2/health/3in1/'+each_name+'urls.csv',index=None)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Device: cpu\n",
      "Processing finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading graph: 100%|██████████| 5315/5315 [00:00<00:00, 170380.28it/s]\n",
      "Training:   0%|          | 0/150600 [00:00<?, ?it/s]/usr/local/lib/python3.7/site-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "Training:  41%|████▏     | 62468/150600 [00:00<00:01, 69386.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'suntimes.com' not in vocabulary\"\n",
      "\"word 'nakedcapitalism.com' not in vocabulary\"\n",
      "\"word 'drugtargetreview.com' not in vocabulary\"\n",
      "\"word 'syrianews.cc' not in vocabulary\"\n",
      "\"word 'brisbanetimes.com.au' not in vocabulary\"\n",
      "\"word 'sciencemag.org' not in vocabulary\"\n",
      "\"word 'anchoragepress.com' not in vocabulary\"\n",
      "\"word 'wsws.org' not in vocabulary\"\n",
      "\"word 'pharmacypracticenews.com' not in vocabulary\"\n",
      "\"word 'taiwannews.com.tw' not in vocabulary\"\n",
      "\"word 'alethonews.com' not in vocabulary\"\n",
      "\"word 'odishatv.in' not in vocabulary\"\n",
      "\"word 'pfizer.com' not in vocabulary\"\n",
      "\"word 'nyoooz.com' not in vocabulary\"\n",
      "\"word 'firstcoastnews.com' not in vocabulary\"\n",
      "\"word 'amerexperience.com' not in vocabulary\"\n",
      "\"word 'upi.com' not in vocabulary\"\n",
      "\"word 'science.news' not in vocabulary\"\n",
      "\"word 'politicsweb.co.za' not in vocabulary\"\n",
      "\"word 'iltalehti.fi' not in vocabulary\"\n",
      "\"word 'suntimes.com' not in vocabulary\"\n",
      "\"word 'blabber.buzz' not in vocabulary\"\n",
      "\"word 'us.org' not in vocabulary\"\n",
      "\"word 'sciencedaily.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'etcnda.com' not in vocabulary\"\n",
      "\"word 'trendnews.eu' not in vocabulary\"\n",
      "\"word 'economictimes.com' not in vocabulary\"\n",
      "\"word 'mississippifreepress.org' not in vocabulary\"\n",
      "\"word 'whio.com' not in vocabulary\"\n",
      "\"word 'australiannationalreview.com' not in vocabulary\"\n",
      "\"word 'taiwannews.com.tw' not in vocabulary\"\n",
      "\"word 'lifesitenews.com' not in vocabulary\"\n",
      "\"word '2gtx.com' not in vocabulary\"\n",
      "\"word 'al.com' not in vocabulary\"\n",
      "\"word 'boston25news.com' not in vocabulary\"\n",
      "\"word 'taiwannews.com.tw' not in vocabulary\"\n",
      "\"word 'newstatesman.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'newsbreak.com' not in vocabulary\"\n",
      "\"word 'newstatesman.com' not in vocabulary\"\n",
      "\"word 'bluebloodz.com' not in vocabulary\"\n",
      "\"word 'bbc.in' not in vocabulary\"\n",
      "\"word 'whio.com' not in vocabulary\"\n",
      "\"word 'anchoragepress.com' not in vocabulary\"\n",
      "\"word 'news18.com' not in vocabulary\"\n",
      "\"word 'dearpandemic.org' not in vocabulary\"\n",
      "\"word 'outbreaknewstoday.com' not in vocabulary\"\n",
      "\"word 'unitaid.org' not in vocabulary\"\n",
      "\"word 'suntimes.com' not in vocabulary\"\n",
      "\"word 'asianetnews.com' not in vocabulary\"\n",
      "\"word 'instagram.com' not in vocabulary\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████▏    | 77510/150600 [00:01<00:01, 72095.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'vancouversun.com' not in vocabulary\"\n",
      "\"word 'buzzsprout.com' not in vocabulary\"\n",
      "\"word 'whio.com' not in vocabulary\"\n",
      "\"word 'fox61.com' not in vocabulary\"\n",
      "\"word 'clevelandclinic.org' not in vocabulary\"\n",
      "\"word 'wreg.com' not in vocabulary\"\n",
      "\"word '2gtx.com' not in vocabulary\"\n",
      "\"word 'newschannel5.com' not in vocabulary\"\n",
      "\"word 'whio.com' not in vocabulary\"\n",
      "\"word 'wsws.org' not in vocabulary\"\n",
      "\"word 'sun-sentinel.com' not in vocabulary\"\n",
      "\"word 'theconversation.com' not in vocabulary\"\n",
      "\"word 'joemygod.com' not in vocabulary\"\n",
      "\"word 'wtop.com' not in vocabulary\"\n",
      "\"word '10tv.com' not in vocabulary\"\n",
      "\"word 'newsday.co.tt' not in vocabulary\"\n",
      "\"word 'eutimes.net' not in vocabulary\"\n",
      "\"word 'wtop.com' not in vocabulary\"\n",
      "\"word 'thephaser.com' not in vocabulary\"\n",
      "\"word 'metro.co.uk' not in vocabulary\"\n",
      "\"word 'wcnc.com' not in vocabulary\"\n",
      "\"word 'wsws.org' not in vocabulary\"\n",
      "\"word 'wsws.org' not in vocabulary\"\n",
      "\"word 'iafrica24.com' not in vocabulary\"\n",
      "\"word 'rumormillnews.com' not in vocabulary\"\n",
      "\"word 'instagram.com' not in vocabulary\"\n",
      "\"word 'businesstoday.in' not in vocabulary\"\n",
      "\"word 'kvoa.com' not in vocabulary\"\n",
      "\"word 'leacountytribune.com' not in vocabulary\"\n",
      "\"word 'chwpeds.com' not in vocabulary\"\n",
      "\"word 'afinalwarning.com' not in vocabulary\"\n",
      "\"word 'rclutz.com' not in vocabulary\"\n",
      "\"word 'realityteam.org' not in vocabulary\"\n",
      "\"word 'clarin.com' not in vocabulary\"\n",
      "\"word 'wentworthreport.com' not in vocabulary\"\n",
      "\"word 'archive.org' not in vocabulary\"\n",
      "\"word 'whio.com' not in vocabulary\"\n",
      "\"word 'lethbridgenewsnow.com' not in vocabulary\"\n",
      "\"word 'lohud.com' not in vocabulary\"\n",
      "\"word 'menshealth.com' not in vocabulary\"\n",
      "\"word 'sci-hub.tw' not in vocabulary\"\n",
      "\"word 'sci-hub.tw' not in vocabulary\"\n",
      "\"word 'myfreedoctor.com' not in vocabulary\"\n",
      "\"word 'canadafreepress.com' not in vocabulary\"\n",
      "\"word 'sciencedaily.com' not in vocabulary\"\n",
      "\"word 'rumormillnews.com' not in vocabulary\"\n",
      "\"word 'farmersweekly.co.za' not in vocabulary\"\n",
      "\"word 'instagram.com' not in vocabulary\"\n",
      "\"word 'reclaimthenet.org' not in vocabulary\"\n",
      "\"word 'myCatholicdoctor.com' not in vocabulary\"\n",
      "\"word 'sciencemag.org' not in vocabulary\"\n",
      "\"word 'medicalnewstoday.com' not in vocabulary\"\n",
      "\"word 'metro.co.uk' not in vocabulary\"\n",
      "\"word 'science.news' not in vocabulary\"\n",
      "\"word 'kshb.com' not in vocabulary\"\n",
      "\"word 'firstcoastnews.com' not in vocabulary\"\n",
      "\"word 'yle.fi' not in vocabulary\"\n",
      "\"word 'oregonstate.edu' not in vocabulary\"\n",
      "\"word 'nanoappsmedical.com' not in vocabulary\"\n",
      "\"word 'anchor.fm' not in vocabulary\"\n",
      "\"word 'nakedcapitalism.com' not in vocabulary\"\n",
      "\"word 'wsws.org' not in vocabulary\"\n",
      "\"word 'suntimes.com' not in vocabulary\"\n",
      "\"word 'economist.com' not in vocabulary\"\n",
      "\"word 'theweek.com' not in vocabulary\"\n",
      "\"word 'cbn.com' not in vocabulary\"\n",
      "\"word 'afinalwarning.com' not in vocabulary\"\n",
      "\"word 'taiwannews.com.tw' not in vocabulary\"\n",
      "\"word 'cleveland.com' not in vocabulary\"\n",
      "\"word 'antiviral.store' not in vocabulary\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 100909/150600 [00:01<00:00, 74514.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'antiviral.store' not in vocabulary\"\n",
      "\"word 'antiviral.store' not in vocabulary\"\n",
      "\"word 'mississippifreepress.org' not in vocabulary\"\n",
      "\"word 'shawlocal.com' not in vocabulary\"\n",
      "\"word 'wosu.pm' not in vocabulary\"\n",
      "\"word 'medicallyspeaking.in' not in vocabulary\"\n",
      "\"word 'chemistryworld.com' not in vocabulary\"\n",
      "\"word 'suntimes.com' not in vocabulary\"\n",
      "\"word 'economictimes.com' not in vocabulary\"\n",
      "\"word 'avaaz.org' not in vocabulary\"\n",
      "\"word 'sciencemag.org' not in vocabulary\"\n",
      "\"word 'hillreporter.com' not in vocabulary\"\n",
      "\"word 'ation-victimes-coronavirus-france.org' not in vocabulary\"\n",
      "\"word 'amazon.com' not in vocabulary\"\n",
      "\"word 'dailytelegraph.com.au' not in vocabulary\"\n",
      "\"word 'ampproject.org' not in vocabulary\"\n",
      "\"word 'politicsweb.co.za' not in vocabulary\"\n",
      "\"word 'anchoragepress.com' not in vocabulary\"\n",
      "\"word 'sj-r.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'theedgemarkets.com' not in vocabulary\"\n",
      "\"word 'stuff.co.nz' not in vocabulary\"\n",
      "\"word 'sj-r.com' not in vocabulary\"\n",
      "\"word 'insider.com' not in vocabulary\"\n",
      "\"word 'wncn.tv' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'lethbridgenewsnow.com' not in vocabulary\"\n",
      "\"word 'metro.co.uk' not in vocabulary\"\n",
      "\"word 'idse.net' not in vocabulary\"\n",
      "\"word 'afinalwarning.com' not in vocabulary\"\n",
      "\"word 'perthnow.com.au' not in vocabulary\"\n",
      "\"word 'wltx.com' not in vocabulary\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 119251/150600 [00:01<00:00, 80993.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'whio.com' not in vocabulary\"\n",
      "\"word 'eastmojo.com' not in vocabulary\"\n",
      "\"word 'cbn.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'freedomsphoenix.com' not in vocabulary\"\n",
      "\"word 'as.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'nine.com.au' not in vocabulary\"\n",
      "\"word 'wsws.org' not in vocabulary\"\n",
      "\"word 'huffpost.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'flip.it' not in vocabulary\"\n",
      "\"word 'sciencemag.org' not in vocabulary\"\n",
      "\"word 'lifeboat.com' not in vocabulary\"\n",
      "\"word 'wsws.org' not in vocabulary\"\n",
      "\"word 'cbs42.com' not in vocabulary\"\n",
      "\"word 'newschannel5.com' not in vocabulary\"\n",
      "\"word 'wsws.org' not in vocabulary\"\n",
      "\"word 'health.gov.au' not in vocabulary\"\n",
      "\"word 'eurweb.com' not in vocabulary\"\n",
      "\"word 'yourbias.is' not in vocabulary\"\n",
      "\"word 'rumormillnews.com' not in vocabulary\"\n",
      "\"word 'toddstarnes.com' not in vocabulary\"\n",
      "\"word 'wosu.pm' not in vocabulary\"\n",
      "\"word 'flip.it' not in vocabulary\"\n",
      "\"word 'sciencemag.org' not in vocabulary\"\n",
      "\"word 'childrenshealthdefense.org' not in vocabulary\"\n",
      "\"word 'cleveland.com' not in vocabulary\"\n",
      "\"word 'leacountytribune.com' not in vocabulary\"\n",
      "\"word 'abplive.com' not in vocabulary\"\n",
      "\"word 'firstcoastnews.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'instagram.com' not in vocabulary\"\n",
      "\"word 'wkbw.com' not in vocabulary\"\n",
      "\"word 'rumormillnews.com' not in vocabulary\"\n",
      "\"word 'sanfordhealth.org' not in vocabulary\"\n",
      "\"word 'wwltv.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'thequint.com' not in vocabulary\"\n",
      "\"word 'finalcall.com' not in vocabulary\"\n",
      "\"word 'uol.com.br' not in vocabulary\"\n",
      "\"word 'patentoffice.ir' not in vocabulary\"\n",
      "\"word 'theconversation.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'pharmacypracticenews.com' not in vocabulary\"\n",
      "\"word 'genengnews.com' not in vocabulary\"\n",
      "\"word 'wtop.com' not in vocabulary\"\n",
      "\"word 'nakedcapitalism.com' not in vocabulary\"\n",
      "\"word 'right.bz' not in vocabulary\"\n",
      "\"word 'citybeat.com' not in vocabulary\"\n",
      "\"word 'statnews.com' not in vocabulary\"\n",
      "\"word 'medicalnewstoday.com' not in vocabulary\"\n",
      "\"word 'wftv.com' not in vocabulary\"\n",
      "\"word 'gov.scot' not in vocabulary\"\n",
      "\"word 'archive.org' not in vocabulary\"\n",
      "\"word 'cnbctv18.com' not in vocabulary\"\n",
      "\"word 'nice.org.uk' not in vocabulary\"\n",
      "\"word '891maxfm.ca' not in vocabulary\"\n",
      "\"word 'reclaimthenet.org' not in vocabulary\"\n",
      "\"word 'kbs.co.kr' not in vocabulary\"\n",
      "\"word 'lifeboat.com' not in vocabulary\"\n",
      "\"word 'farmersweekly.co.za' not in vocabulary\"\n",
      "\"word 'news4sanantonio.com' not in vocabulary\"\n",
      "\"word 'ibtimes.co.in' not in vocabulary\"\n",
      "\"word 'miragenews.com' not in vocabulary\"\n",
      "\"word 'cleveland.com' not in vocabulary\"\n",
      "\"word 'krem.com' not in vocabulary\"\n",
      "\"word 'anchor.fm' not in vocabulary\"\n",
      "\"word 'taiwannews.com.tw' not in vocabulary\"\n",
      "\"word 'suntimes.com' not in vocabulary\"\n",
      "\"word 'realityteam.org' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'entityart.co.uk' not in vocabulary\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 136322/150600 [00:01<00:00, 77590.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"word 'pharmacyonair.com' not in vocabulary\"\n",
      "\"word 'heavy.com' not in vocabulary\"\n",
      "\"word 'ivermectinforcovid.store' not in vocabulary\"\n",
      "\"word 'threadreaderapp.com' not in vocabulary\"\n",
      "\"word 'cbn.com' not in vocabulary\"\n",
      "\"word 'archive.org' not in vocabulary\"\n",
      "\"word 'cnb.cx' not in vocabulary\"\n",
      "\"word 'pennlive.com' not in vocabulary\"\n",
      "\"word 'wtop.com' not in vocabulary\"\n",
      "\"word 'bvsalud.org' not in vocabulary\"\n",
      "\"word 'suntimes.com' not in vocabulary\"\n",
      "\"word 'buzzsprout.com' not in vocabulary\"\n",
      "\"word 'nakedcapitalism.com' not in vocabulary\"\n",
      "\"word 'wkbw.com' not in vocabulary\"\n",
      "\"word 'clarin.com' not in vocabulary\"\n",
      "\"word 'shr.lc' not in vocabulary\"\n",
      "\"word 'covid19india.org' not in vocabulary\"\n",
      "\"word 'haaretz.com' not in vocabulary\"\n",
      "\"word 'theglobeandmail.com' not in vocabulary\"\n",
      "\"word 'basedunderground.com' not in vocabulary\"\n",
      "\"word 'wtop.com' not in vocabulary\"\n",
      "\"word 'newshub.lk' not in vocabulary\"\n",
      "\"word 'perthnow.com.au' not in vocabulary\"\n",
      "\"word 'health.gov.au' not in vocabulary\"\n",
      "\"word 'newstarget.com' not in vocabulary\"\n",
      "\"word '20minutos.es' not in vocabulary\"\n",
      "\"word 'kshb.com' not in vocabulary\"\n",
      "\"word 'ksdk.com' not in vocabulary\"\n",
      "\"word 'krem.com' not in vocabulary\"\n",
      "\"word 'nanoappsmedical.com' not in vocabulary\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 144315/150600 [00:01<00:00, 78248.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'odishatv.in' not in vocabulary\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 150600/150600 [00:02<00:00, 52473.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"word 'insider.com' not in vocabulary\"\n",
      "\"word '13wham.com' not in vocabulary\"\n",
      "\"word 'suntimes.com' not in vocabulary\"\n",
      "\"word 'proboards.com' not in vocabulary\"\n",
      "\"word 'sciencemag.org' not in vocabulary\"\n",
      "\"word 'firstcoastnews.com' not in vocabulary\"\n",
      "\"word 'reseauinternational.net' not in vocabulary\"\n",
      "\"word 'suntimes.com' not in vocabulary\"\n",
      "\"word 'economist.com' not in vocabulary\"\n",
      "\"word 'sj-r.com' not in vocabulary\"\n",
      "\"word 'basedunderground.com' not in vocabulary\"\n",
      "\"word 'StudyFinds.org' not in vocabulary\"\n",
      "\"word 'wtop.com' not in vocabulary\"\n",
      "\"word 'reclaimthenet.org' not in vocabulary\"\n",
      "\"word 'sanyonews.jp' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'entityart.co.uk' not in vocabulary\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word '2gtx.com' not in vocabulary\"\n",
      "\"word 'thequint.com' not in vocabulary\"\n",
      "\"word 'cjonline.com' not in vocabulary\"\n",
      "\"word 'themuslimtimes.info' not in vocabulary\"\n",
      "\"word 'wgrz.com' not in vocabulary\"\n",
      "\"word 'irishtimes.com' not in vocabulary\"\n",
      "\"word 'medicalnewstoday.com' not in vocabulary\"\n",
      "\"word 'whatsapp.com' not in vocabulary\"\n",
      "\"word 'cbs42.com' not in vocabulary\"\n",
      "\"word 'daily-sun.com' not in vocabulary\"\n",
      "\"word 'news4sanantonio.com' not in vocabulary\"\n",
      "\"word 'businesstoday.in' not in vocabulary\"\n",
      "\"word 'lcsun-news.com' not in vocabulary\"\n",
      "\"word 'theweek.com' not in vocabulary\"\n",
      "\"word 'firstpost.com' not in vocabulary\"\n",
      "\"word 'lohud.com' not in vocabulary\"\n",
      "\"word 'nakedcapitalism.com' not in vocabulary\"\n",
      "\"word 'thelancet.com' not in vocabulary\"\n",
      "\"word 'statnews.com' not in vocabulary\"\n",
      "\"word 'stuff.co.nz' not in vocabulary\"\n",
      "\"word '2gtx.com' not in vocabulary\"\n",
      "\"word 'drugtargetreview.com' not in vocabulary\"\n",
      "\"word 'newstarget.com' not in vocabulary\"\n",
      "\"word 'yourtango.com' not in vocabulary\"\n",
      "\"word 'yourworldevents.com' not in vocabulary\"\n",
      "\"word 'us.org' not in vocabulary\"\n",
      "\"word '2gtx.com' not in vocabulary\"\n",
      "\"word 'wftv.com' not in vocabulary\"\n",
      "\"word 'sciencemag.org' not in vocabulary\"\n",
      "\"word 'sciencemag.org' not in vocabulary\"\n",
      "\"word 'wkbw.com' not in vocabulary\"\n",
      "\"word 'theedgemarkets.com' not in vocabulary\"\n",
      "\"word 'whio.com' not in vocabulary\"\n",
      "\"word 'clinicaltrialsarena.com' not in vocabulary\"\n",
      "\"word 'lifeboat.com' not in vocabulary\"\n",
      "\"word 'nbc6.com' not in vocabulary\"\n",
      "\"word 'lcsun-news.com' not in vocabulary\"\n",
      "\"word 'sciencedaily.com' not in vocabulary\"\n",
      "\"word 'wfp.to' not in vocabulary\"\n",
      "\"word 'amazon.com' not in vocabulary\"\n",
      "\"word 'bbc.in' not in vocabulary\"\n",
      "\"word 'politicsweb.co.za' not in vocabulary\"\n",
      "\"word 'lifeboat.com' not in vocabulary\"\n",
      "\"word 'wikipedia.org' not in vocabulary\"\n",
      "\"word 'businesslive.co.za' not in vocabulary\"\n",
      "\"word 'nakedcapitalism.com' not in vocabulary\"\n",
      "\"word 'economictimes.com' not in vocabulary\"\n",
      "\"word 'toddstarnes.com' not in vocabulary\"\n",
      "\"word 'flip.it' not in vocabulary\"\n",
      "\"word 'macon.com' not in vocabulary\"\n",
      "\"word 'businesstoday.in' not in vocabulary\"\n",
      "\"word 'ampproject.org' not in vocabulary\"\n",
      "\"word 'archive.org' not in vocabulary\"\n",
      "\"word 'meaww.com' not in vocabulary\"\n",
      "\"word 'wsws.org' not in vocabulary\"\n",
      "\"word 'enca.com' not in vocabulary\"\n",
      "\"word 'mol.im' not in vocabulary\"\n",
      "\"word 'statnews.com' not in vocabulary\"\n",
      "\"word 'eastmojo.com' not in vocabulary\"\n",
      "\"word 'wosu.pm' not in vocabulary\"\n",
      "\"word 'anchor.fm' not in vocabulary\"\n",
      "\"word 'wsws.org' not in vocabulary\"\n",
      "\"word 'cbsloc.al' not in vocabulary\"\n",
      "\"word 'wirenews.org' not in vocabulary\"\n",
      "\"word 'jesuismalade.org' not in vocabulary\"\n",
      "\"word 'threader.app' not in vocabulary\"\n",
      "\"word 'nanoappsmedical.com' not in vocabulary\"\n",
      "\"word 'lbry.tv' not in vocabulary\"\n",
      "\"word 'meaww.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'anchoragepress.com' not in vocabulary\"\n",
      "\"word 'docdroid.net' not in vocabulary\"\n",
      "\"word '20minutes.fr' not in vocabulary\"\n",
      "\"word 'sfgate.com' not in vocabulary\"\n",
      "\"word 'citybeat.com' not in vocabulary\"\n",
      "\"word 'koin.com' not in vocabulary\"\n",
      "\"word 'lethbridgenewsnow.com' not in vocabulary\"\n",
      "\"word 'antiviral.store' not in vocabulary\"\n",
      "\"word 'wbay.com' not in vocabulary\"\n",
      "\"word 'shawlocal.com' not in vocabulary\"\n",
      "\"word 'theweek.com' not in vocabulary\"\n",
      "\"word 'livescience.com' not in vocabulary\"\n",
      "\"word 'ktsm.com' not in vocabulary\"\n",
      "\"word 'sciencemag.org' not in vocabulary\"\n",
      "\"word 'theburningplatform.com' not in vocabulary\"\n",
      "\"word 'wltx.com' not in vocabulary\"\n",
      "\"word 'lifeboat.com' not in vocabulary\"\n",
      "\"word 'citybeat.com' not in vocabulary\"\n",
      "\"word 'patreon.com' not in vocabulary\"\n",
      "\"word 'podbean.com' not in vocabulary\"\n",
      "\"word 'wwltv.com' not in vocabulary\"\n",
      "\"word '808ne.ws' not in vocabulary\"\n",
      "\"word 'sj-r.com' not in vocabulary\"\n",
      "\"word 'bbc.in' not in vocabulary\"\n",
      "\"word 'boards.net' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'lohud.com' not in vocabulary\"\n",
      "\"word 'yle.fi' not in vocabulary\"\n",
      "\"word 'statnews.com' not in vocabulary\"\n",
      "\"word 'eutimes.net' not in vocabulary\"\n",
      "\"word 'dearpandemic.org' not in vocabulary\"\n",
      "\"word 'thequint.com' not in vocabulary\"\n",
      "\"word 'wltx.com' not in vocabulary\"\n",
      "\"word 'lifeboat.com' not in vocabulary\"\n",
      "\"word 'healthgrades.com' not in vocabulary\"\n",
      "\"word 'ijidonline.com' not in vocabulary\"\n",
      "\"word 'theconversation.com' not in vocabulary\"\n",
      "\"word 'asianetnews.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'health.gov.au' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'nakedcapitalism.com' not in vocabulary\"\n",
      "\"word 'sciencemag.org' not in vocabulary\"\n",
      "\"word 'koin.com' not in vocabulary\"\n",
      "\"word 'medicalnewstoday.com' not in vocabulary\"\n",
      "\"word 'newschannel5.com' not in vocabulary\"\n",
      "\"word 'eutimes.net' not in vocabulary\"\n",
      "\"word 'odishatv.in' not in vocabulary\"\n",
      "\"word 'insider.com' not in vocabulary\"\n",
      "\"word 'whio.com' not in vocabulary\"\n",
      "\"word 'enca.com' not in vocabulary\"\n",
      "\"word 'sciencemag.org' not in vocabulary\"\n",
      "\"word 'practiceupdate.com' not in vocabulary\"\n",
      "\"word 'flip.it' not in vocabulary\"\n",
      "\"word 'anchor.fm' not in vocabulary\"\n",
      "\"word 'yle.fi' not in vocabulary\"\n",
      "\"word 'sciencemag.org' not in vocabulary\"\n",
      "\"word 'cleveland.com' not in vocabulary\"\n",
      "\"word 'cleveland.com' not in vocabulary\"\n",
      "\"word 'newstatesman.com' not in vocabulary\"\n",
      "\"word 'lcsun-news.com' not in vocabulary\"\n",
      "\"word 'news5cleveland.com' not in vocabulary\"\n",
      "\"word 'nbcchicago.com' not in vocabulary\"\n",
      "\"word 'mol.im' not in vocabulary\"\n",
      "\"word 'farmersweekly.co.za' not in vocabulary\"\n",
      "\"word 'usa366.com' not in vocabulary\"\n",
      "\"word 'economist.com' not in vocabulary\"\n",
      "\"word 'ivermectinforcovid.store' not in vocabulary\"\n",
      "\"word 'sciencemag.org' not in vocabulary\"\n",
      "\"word 'lifesitenews.com' not in vocabulary\"\n",
      "\"word 'huffpost.com' not in vocabulary\"\n",
      "\"word 'newsbreak.com' not in vocabulary\"\n",
      "\"word 'tribune.net.ph' not in vocabulary\"\n",
      "\"word 'newshub.lk' not in vocabulary\"\n",
      "\"word 'theedgemarkets.com' not in vocabulary\"\n",
      "\"word 'ksdk.com' not in vocabulary\"\n",
      "\"word 'al.com' not in vocabulary\"\n",
      "\"word 'health.gov.au' not in vocabulary\"\n",
      "\"word 'politicsweb.co.za' not in vocabulary\"\n",
      "\"word 'whio.com' not in vocabulary\"\n",
      "\"word 'shr.lc' not in vocabulary\"\n",
      "\"word 'saidit.net' not in vocabulary\"\n",
      "\"word 'cbsnews.com' not in vocabulary\"\n",
      "\"word 'flip.it' not in vocabulary\"\n",
      "\"word 'faqcheck.org' not in vocabulary\"\n",
      "\"word 'suntimes.com' not in vocabulary\"\n",
      "\"word 'odishatv.in' not in vocabulary\"\n",
      "\"word 'bbc.in' not in vocabulary\"\n",
      "\"word 'dearpandemic.org' not in vocabulary\"\n",
      "\"word 'nakedcapitalism.com' not in vocabulary\"\n",
      "\"word 'chwpeds.com' not in vocabulary\"\n",
      "\"word 'wirenews.org' not in vocabulary\"\n",
      "\"word 'wirenews.org' not in vocabulary\"\n",
      "\"word 'cleveland.com' not in vocabulary\"\n",
      "\"word 'marketwatch.com' not in vocabulary\"\n",
      "\"word 'cbs42.com' not in vocabulary\"\n",
      "\"word 'nbc6.com' not in vocabulary\"\n",
      "\"word 'suntimes.com' not in vocabulary\"\n",
      "\"word 'daily-sun.com' not in vocabulary\"\n",
      "\"word 'lcsun-news.com' not in vocabulary\"\n",
      "\"word 'themalaysianreserve.com' not in vocabulary\"\n",
      "\"word 'norwaytoday.info' not in vocabulary\"\n",
      "\"word 'king5.com' not in vocabulary\"\n",
      "\"word 'suntimes.com' not in vocabulary\"\n",
      "\"word 'lcsun-news.com' not in vocabulary\"\n",
      "\"word 'newschannel5.com' not in vocabulary\"\n",
      "\"word 'oregonstate.edu' not in vocabulary\"\n",
      "\"word 'odishatv.in' not in vocabulary\"\n",
      "\"word 'sciencedaily.com' not in vocabulary\"\n",
      "\"word 'wentworthreport.com' not in vocabulary\"\n",
      "\"word 'sciencedaily.com' not in vocabulary\"\n",
      "\"word 'news18.com' not in vocabulary\"\n",
      "\"word 'kxlh.com' not in vocabulary\"\n",
      "\"word 'wwltv.com' not in vocabulary\"\n",
      "\"word 'realityteam.org' not in vocabulary\"\n",
      "\"word 'wsws.org' not in vocabulary\"\n",
      "\"word 'clevelandclinic.org' not in vocabulary\"\n",
      "\"word 'farmersweekly.co.za' not in vocabulary\"\n",
      "\"word 'pharmacychecker.com' not in vocabulary\"\n",
      "\"word 'aapsonline.org' not in vocabulary\"\n",
      "\"word 'odishatv.in' not in vocabulary\"\n",
      "\"word 'gov.scot' not in vocabulary\"\n",
      "\"word 'lohud.com' not in vocabulary\"\n",
      "\"word 'gov.scot' not in vocabulary\"\n",
      "\"word 'timeslive.co.za' not in vocabulary\"\n",
      "\"word 'wirenews.org' not in vocabulary\"\n",
      "\"word 'nj.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'contagionlive.com' not in vocabulary\"\n",
      "\"word 'odishatv.in' not in vocabulary\"\n",
      "\"word 'medicalnewstoday.com' not in vocabulary\"\n",
      "\"word 'newstatesman.com' not in vocabulary\"\n",
      "\"word 'anchoragepress.com' not in vocabulary\"\n",
      "\"word 'iono.fm' not in vocabulary\"\n",
      "\"word 'blabber.buzz' not in vocabulary\"\n",
      "\"word 'cnbctv18.com' not in vocabulary\"\n",
      "\"word 'odishatv.in' not in vocabulary\"\n",
      "\"word 'ampproject.org' not in vocabulary\"\n",
      "\"word 'chwpeds.com' not in vocabulary\"\n",
      "\"word 'theedgemarkets.com' not in vocabulary\"\n",
      "\"word 'mdpi.com' not in vocabulary\"\n",
      "\"word 'entityart.co.uk' not in vocabulary\"\n",
      "\"word 'lohud.com' not in vocabulary\"\n",
      "\"word 'koin.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'bbc.in' not in vocabulary\"\n",
      "\"word 'health.gov.au' not in vocabulary\"\n",
      "\"word 'wkbw.com' not in vocabulary\"\n",
      "\"word 'wsws.org' not in vocabulary\"\n",
      "\"word 'ktvb.com' not in vocabulary\"\n",
      "\"word 'sciencedaily.com' not in vocabulary\"\n",
      "\"word 'suntimes.com' not in vocabulary\"\n",
      "\"word 'lcsun-news.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'huffpost.com' not in vocabulary\"\n",
      "\"word 'newschannel5.com' not in vocabulary\"\n",
      "\"word 'rokfin.com' not in vocabulary\"\n",
      "\"word 'flip.it' not in vocabulary\"\n",
      "\"word 'patreon.com' not in vocabulary\"\n",
      "\"word 'abcnews4.com' not in vocabulary\"\n",
      "\"word 'sci-hub.tw' not in vocabulary\"\n",
      "\"word 'sciencemag.org' not in vocabulary\"\n",
      "\"word 'as.com' not in vocabulary\"\n",
      "\"word 'sciencemag.org' not in vocabulary\"\n",
      "\"word 'citybeat.com' not in vocabulary\"\n",
      "\"word '2gtx.com' not in vocabulary\"\n",
      "\"word 'gizmodo.com' not in vocabulary\"\n",
      "\"word 'kansascity.com' not in vocabulary\"\n",
      "\"word 'lifeboat.com' not in vocabulary\"\n",
      "\"word 'lcsun-news.com' not in vocabulary\"\n",
      "\"word 'statnews.com' not in vocabulary\"\n",
      "\"word 'taiwannews.com.tw' not in vocabulary\"\n",
      "\"word 'theburningplatform.com' not in vocabulary\"\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "#from scipy.spatial.distance import cdist\n",
    "\n",
    "#similarity score concat before fc1 \n",
    "import tldextract\n",
    "from itertools import combinations\n",
    "from shutil import which\n",
    "import torch.nn.functional as F\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from cleantext import clean\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import preprocessor as p \n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import LongformerModel, LongformerTokenizer, LongformerConfig\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"my bar!\")\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "#import wandb\n",
    "#import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "import gensim.models as gsm\n",
    "\n",
    "\n",
    "\n",
    "import emoji #pip install emoji --upgrade\n",
    "# Internal dependencies\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1, 2, 3' \n",
    "#....to install apex...\n",
    "#pip install -v --no-cache-dir ./\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "#from torchnlp.nn import Attention #pip imstall pytorch-nlp\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "#from fastnode2vec import Graph, Node2Vec\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from cleantext import clean\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import preprocessor as p \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import LongformerModel, LongformerTokenizer, LongformerConfig\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"my bar!\")\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "#import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex \n",
    "#pip install emoji --upgrade\n",
    "# Internal dependencies\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributed as dist\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "from nltk import bigrams\n",
    "\n",
    "\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1, 2, 3' \n",
    "#....to install apex...\n",
    "#pip install -v --no-cache-dir ./\n",
    "\n",
    "def extract_domain(x):\n",
    "    '''\n",
    "    e.g. x='http://www.bbc.co.uk'\n",
    "    '''\n",
    "    ext = tldextract.extract(x)\n",
    "    return ext.registered_domain\n",
    "\n",
    "def retrieveRTMT(_df,n2vRT,n2vMT):\n",
    "    '''\n",
    "    #_names: nodes' name in RT or MT network\n",
    "    '''\n",
    "    \n",
    "    dfRT=pd.DataFrame(n2vRT.wv.vectors,index=n2vRT.wv.index2entity)\n",
    "    dfMT=pd.DataFrame(n2vMT.wv.vectors,index=n2vMT.wv.index2entity)\n",
    "    RT=dfRT[dfRT.index.isin(_df['user_screen_name'])].values\n",
    "    MT=dfMT[dfMT.index.isin(_df['user_screen_name'])].values\n",
    "    \n",
    "    \n",
    "    RT=torch.tensor(RT)\n",
    "    MT=torch.tensor(MT)\n",
    "    return RT, MT\n",
    "\n",
    "def prepro(ndf,col):\n",
    "    \n",
    "    import re\n",
    "    def remove_rt(query):\n",
    "        query=re.sub(r\"\\brt\", \"\", query)\n",
    "        return query\n",
    "    ndf[col]=ndf[col].progress_apply(lambda x: remove_rt(x))\n",
    "\n",
    "    def tokenizer(_text):\n",
    "        return TweetTokenizer().tokenize(_text)\n",
    "\n",
    "    ndf[col]=ndf[col].progress_apply(lambda x:tokenizer(x))\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    STOPWORDS=stopwords.words('english')\n",
    "    def remove_stop(_text):\n",
    "        return ' '.join([x for x in _text if x not in STOPWORDS])\n",
    "\n",
    "    ndf[col]=ndf[col].progress_apply(lambda x: remove_stop(x))\n",
    "    \n",
    "    def preprocessing(string): \n",
    "\n",
    "        text=clean(string,lower=True, no_emails=True,no_numbers=True,no_punct=True,no_digits=False,no_currency_symbols=True,\n",
    "                   replace_with_number=\"\",\n",
    "                   no_urls=True,replace_with_url=\"\",\n",
    "                   replace_with_email=\"\",\n",
    "                   replace_with_currency_symbol=\"\")\n",
    "\n",
    "        return text\n",
    "    ndf[col]=ndf[col].progress_apply(lambda x: preprocessing(x))\n",
    "    \n",
    "    def remove_emoji(string):\n",
    "        p.set_options(p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "        \n",
    "        return p.clean(string)\n",
    "    ndf[col]=ndf[col].progress_apply(lambda x: remove_emoji(x))\n",
    "    \n",
    "    ndf.reset_index(inplace=True,drop=True)\n",
    "    return ndf\n",
    "\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.enabled = False\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "def makefolder(save):\n",
    "    if  os.path.exists(save) == False:\n",
    "        os.makedirs(save)\n",
    "    else:\n",
    "        print('Floder:',save,'Exsit!')\n",
    "    return save\n",
    "def get_intersection(_df,_intersecion):\n",
    "    for each  in ['source','target']:\n",
    "\n",
    "        _df=_df[_df[each].isin(_intersecion)]\n",
    "    return _df\n",
    "\n",
    "def fast_node2vec(_df,_save,_file,saving_name,epochs):\n",
    "    '''\n",
    "    _df:retweetdf,mentiondf\n",
    "    _save:saving folder path\n",
    "    _file:csv name, e.g., vaccine, mask\n",
    "    saving_name: RT or MT\n",
    "    '''\n",
    "    \n",
    "    _tuples=_df.to_records(index=False)\n",
    "\n",
    "    _lst=list(_tuples)\n",
    "\n",
    "    _graph = Graph(_lst, directed=True, weighted=True)\n",
    "\n",
    "    _n2v = Node2Vec(_graph, dim=300, walk_length=100, context=10, p=2.0, q=0.5, workers=-1)\n",
    "    \n",
    "    _n2v.train(epochs=epochs)\n",
    "\n",
    "    _n2v.wv.save(_save+_file+'_'+saving_name+'.wv')\n",
    "    return _n2v\n",
    "\n",
    "def updatecol(df1,df2,colname,index):\n",
    "    df1[colname]=''\n",
    "\n",
    "    df1.set_index(index,inplace=True)\n",
    "\n",
    "    df2.set_index(index,inplace=True)\n",
    "\n",
    "    df1.update(df2)\n",
    "\n",
    "    df1.reset_index(inplace=True)\n",
    "    df2.reset_index(inplace=True)\n",
    "    return df1\n",
    "\n",
    "def updatecol_seperate(df1,df2,colname,index1,index2):\n",
    "    df1[colname]=''\n",
    "\n",
    "    df1.set_index(index1,inplace=True)\n",
    "\n",
    "    df2.set_index(index2,inplace=True)\n",
    "\n",
    "    df1.update(df2)\n",
    "\n",
    "    df1.reset_index(inplace=True)\n",
    "    df2.reset_index(inplace=True)\n",
    "    return df1\n",
    "\n",
    "\n",
    "\n",
    "def preproNetwork(_df,nettype,topn,Need,_name):\n",
    "    '''\n",
    "    target: 'retweeted_status_screen_name' or 'user_mentions_screen_names'\n",
    "    return topn indegree dataframe\n",
    "    _name: QAnon or Biden\n",
    "    '''\n",
    "    if nettype=='RT':\n",
    "        target= 'retweeted_status_screen_name' \n",
    "    elif nettype=='MT':\n",
    "        target= 'user_mentions_screen_names'\n",
    "        \n",
    "    file=_name\n",
    "    save='../Project2/health/'+file+'_data_processed/'\n",
    "    makefolder(save)\n",
    "        \n",
    "    #_df=_df.rename(columns={'user_screen_name':'users'})\n",
    "    _df=_df[['user_screen_name',target]]\n",
    "    _df.dropna(subset=[target],inplace=True)\n",
    "    _df[target]=_df[target].str.replace('\\'','')\n",
    "    _df[target]=_df[target].apply(lambda x: x.split(','))\n",
    "    _df=_df.explode([target])\n",
    "    _df=_df[['user_screen_name',target]]\n",
    "    #_df.columns=['source','target']\n",
    "    \n",
    "    \n",
    "    _df=_df.explode([target])\n",
    "    \n",
    "    G=nx.from_pandas_edgelist(_df,source='user_screen_name',target=target,create_using=nx.DiGraph())\n",
    "\n",
    "    nx.write_gexf(G, save+nettype+ '.gexf')\n",
    "    mdf=pd.DataFrame(G.in_degree)\n",
    "    mdf.columns=['Allusers','indegree']\n",
    "    kdf=mdf.sort_values('indegree',ascending=False).head(topn)\n",
    "    kdf.to_csv('../Project2/health/top'+str(topn)+nettype+'indegree.csv',index=None)\n",
    "    #print(_df.columns)\n",
    "    #print(kdf.columns)\n",
    "    ndf=updatecol_seperate(_df,kdf,'indegree','user_screen_name','Allusers')\n",
    "    ldf=ndf[ndf['indegree']!='']\n",
    "    ldf['indegree']=ldf['indegree'].astype(int)\n",
    "    ldf=ldf.sort_values('indegree',ascending=False)\n",
    "    \n",
    "    #produce node2vec\n",
    "    if Need==True:\n",
    "        __df=_df.groupby(['user_screen_name',target]).size().reset_index()\n",
    "        __df=__df.rename(columns={0:'weight'})\n",
    "        n2v=fast_node2vec(__df, save, file+'_'+nettype+'_weighted',nettype,500)\n",
    "    else:\n",
    "        n2v=None\n",
    "    return ldf, n2v\n",
    "\n",
    "def rank_indegree(_df1,_indegree,usertypes,rank):\n",
    "    _df1p=updatecol_seperate(_df1,_indegree,'indegree',usertypes[rank],'user_screen_name')\n",
    "    _df1p['indegree']=pd.to_numeric(_df1p['indegree'])\n",
    "    #df1p['indegree']=df1p['indegree'].astype(int)\n",
    "    _df1p=_df1p.sort_values('indegree',ascending=False).drop_duplicates(usertypes[rank])\n",
    "    #print(df1p.shape)\n",
    "    return _df1p\n",
    "\n",
    "\n",
    "def get_textdf(_df):\n",
    "    #_df=_df[['user_screen_name','text','news']]\n",
    "    #_df=_df.groupby(by='user_screen_name').agg(text=(\"text\", lambda x: \",\".join(set(x))))\n",
    "    #_df.reset_index(inplace=True)\n",
    "    \n",
    "    _t1=_df.groupby(by='user_screen_name').agg(text=(\"text\", lambda x: \",\".join(set(x))))\n",
    "\n",
    "    _t2=_df.groupby(by='user_screen_name').agg(news=(\"news\", lambda x: \",\".join(set(x))))\n",
    "    \n",
    "    _t1['news']=''\n",
    "    \n",
    "    _t1.update(_t2)\n",
    "    \n",
    "    _t1.reset_index(inplace=True)\n",
    "    return _t1\n",
    "\n",
    "class cos_feature:\n",
    "    def __init__(self, text, news):\n",
    "        self.long_roberta = long_roberta\n",
    "        self.text = text\n",
    "        self.news = news\n",
    "\n",
    "    def cos_similarity(self):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = self.long_roberta(self.text)[0]\n",
    "        return embedding\n",
    "\n",
    "    def emb_similarity(self):\n",
    "\n",
    "        emb1 = self.cos_similarity(self.text)\n",
    "        emb2 = self.cos_similarity(self.news)\n",
    "        emb1 = emb1.cpu().detach().numpy()\n",
    "        emb2  = emb2.cpu().detach().numpy()\n",
    "        print('tweet:',emb1.shape)\n",
    "        print('news:', emb2.shape)\n",
    "        Y = cdist(emb1[0], emb2[0], 'cosine')[0][0]\n",
    "        cos_sim = 1.0 - Y\n",
    "        print('similarity:',cos_sim)\n",
    "        return cos_sim\n",
    "\n",
    "class ROBERTALSTMSentiment(nn.Module):\n",
    "        def __init__(self,\n",
    "                    long_roberta,\n",
    "                    hidden_dim,\n",
    "                    output_dim,\n",
    "                    n_layers,\n",
    "                    bidirectional,\n",
    "                    dropout):\n",
    "            \n",
    "            super().__init__()\n",
    "            \n",
    "            self.long_roberta = long_roberta\n",
    "            \n",
    "            embedding_dim = long_roberta.config.to_dict()['hidden_size'] #768\n",
    "                \n",
    "            self.rnn = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers = n_layers,\n",
    "                            bidirectional = bidirectional,\n",
    "                            batch_first = True,\n",
    "                            dropout = 0 if n_layers < 2 else dropout)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.attn_fc = Attention(hidden_dim * 2 if bidirectional else hidden_dim) #attention layer from torchnlp\n",
    "            self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "        def forward(self, text):\n",
    "            with torch.no_grad():\n",
    "                embedded = self.long_roberta(text)[0]\n",
    "            lstm_out, (hidden, c_n) = self.rnn(embedded)\n",
    "\n",
    "            if self.rnn.bidirectional: #add dropout\n",
    "                hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "            else:\n",
    "                hidden = self.dropout(hidden[-1,:,:])\n",
    "  \n",
    "            attn_out = self.attn_fc(hidden.unsqueeze(1), lstm_out)\n",
    "           \n",
    "            return attn_out[0].squeeze(1)\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "def token_id_twt(sentences):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        # For every sentence...\n",
    "        for sent in sentences:\n",
    "            # `encode_plus` will:\n",
    "            #   (1) Tokenize the sentence.\n",
    "            #   (2) Prepend the `[CLS]` token to the start.\n",
    "            #   (3) Append the `[SEP]` token to the end.\n",
    "            #   (4) Map tokens to their IDs.\n",
    "            #   (5) Pad or truncate the sentence to `max_length`\n",
    "            #   (6) Create attention masks for [PAD] tokens.\n",
    "            encoded_dict = longformer_tokenizer.encode_plus(\n",
    "                                sent,                      # Sentence to encode.\n",
    "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                max_length = 4000,           # Pad & truncate all sentences.\n",
    "                                pad_to_max_length = True,\n",
    "                                return_attention_mask = True,   # Construct attn. masks.\n",
    "                                truncation=True, #explicitely truncate examples to max length. #my add\n",
    "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        )\n",
    "\n",
    "            # Add the encoded sentence to the list.    \n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            #print(type(input_ids[0]), input_ids[0], input_ids[0].size())\n",
    "            #sys.exit()\n",
    "\n",
    "            # And its attention mask (simply differentiates padding from non-padding).\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        # Convert the lists into tensors.\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "        #labels = torch.tensor(labels)\n",
    "\n",
    "    #     # Print sentence 0, now as a list of IDs.\n",
    "    #     print('Original: ', sentences_train[0])\n",
    "    #     print('Token IDs:', input_ids_train[0])\n",
    "        return input_ids, attention_masks\n",
    "\n",
    "class NetworkMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkMLP, self).__init__() \n",
    "        self.fc1 = nn.Linear(300, 150)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        z1 = self.fc1(X)\n",
    "        return z1 \n",
    "\n",
    "class JointModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.model_net = NetworkMLP()\n",
    "        self.model_twt = ROBERTALSTMSentiment(long_roberta, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "        self.fc1 = nn.Linear(600, 600)\n",
    "        self.fc2 = nn.Linear(600, output_dim) #original\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x_t): \n",
    "        prediction_twt = self.model_twt(x_t)\n",
    "        #prediction_twtNews = self.model_twt(x_tNews)\n",
    "       \n",
    "        #concat_pred = torch.cat((prediction_twt, prediction_twtNews), 1)\n",
    "        output = self.fc1(prediction_twt)\n",
    "        \n",
    "        output = self.dropout(output) # add dropout\n",
    "        output = self.fc2(F.relu(output)) #original\n",
    "        #output = self.dropout(output)#add dropout\n",
    "        #output = self.fc3(F.relu(output))\n",
    "        return output\n",
    "        \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    #rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    #rounded_preds = torch.round(preds)\n",
    "    correct = (preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    \n",
    "    from sklearn.metrics import f1_score\n",
    "    macro_f1 = f1_score(y.to(\"cpu\"), preds.to(\"cpu\"), average='macro')\n",
    "    #print(\"macro_f1\", macro_f1)\n",
    "\n",
    "    return acc, macro_f1\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(iterator):    \n",
    "    #for batch in iterator:\n",
    "        #print(\"batch\", batch)\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        #   b_input_ids = batch[0].to(device)\n",
    "        #   b_input_mask = batch[1].to(device)\n",
    "        #   b_labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #netRT = batch[0].to(device)\n",
    "        #netMT = batch[1].to(device)\n",
    "        #feature1=batch[0]\n",
    "        #print('feature1:',feature1)\n",
    "        #feature2=batch[1]\n",
    "        #cosine_score=emb_similarity(feature1, feature2)\n",
    "        #cosine_score_view=cosine_score.view(cosine_score.size()[0],1)\n",
    "\n",
    "        twt = batch[0].to(device)\n",
    "        #news= batch[1].to(device)\n",
    "\n",
    "        #print('text.size:',twt.size())\n",
    "        #print('news.size:',news.size())\n",
    "\n",
    "        #cosine_similarity=cosine_similarity.to(device)\n",
    "        label = batch[1].to(device)\n",
    "\n",
    "        #print(\"label\", label, type(label),label.size()) #torch.Size([32])\n",
    "        #label = label.unsqueeze(1)\n",
    "        #print(\"label\", label, type(label),label.size())\n",
    "        #predictions = model(batch.text).squeeze(1)\n",
    "        #predictions = model(text)\n",
    "        predictions = model(twt)#,cosine_similarity)\n",
    "\n",
    "        #cosine_score_view=cosine_score.view(cosine_score.size()[0],1)\n",
    "\n",
    "\n",
    "\n",
    "        #print('cosine_similarity.size:',cosine_score.size())\n",
    "        #predictions=torch.cat((predictions,cosine_score_view), dim = 1)\n",
    "        #print('predictions.size:',predictions.size())\n",
    "        loss = criterion(predictions, label)\n",
    "    \n",
    "        acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_macro += macro_f1.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        #for batch in iterator:\n",
    "        for step, batch in enumerate(iterator):    \n",
    "            #des = batch[0].to(device)\n",
    "            #loc = batch[1].to(device)\n",
    "            #netRT = batch[0].to(device)\n",
    "            #netMT = batch[1].to(device)\n",
    "            twt = batch[0].to(device)\n",
    "            #news= batch[1].to(devifce)\n",
    "            label = batch[1].to(device)\n",
    "            #predictions = model(batch.text).squeeze(1)\n",
    "            predictions = model(twt)\n",
    "            #loss = criterion(predictions, batch.label)\n",
    "            loss = criterion(predictions, label)\n",
    "            #acc = binary_accuracy(predictions, batch.label)\n",
    "            acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_macro += macro_f1.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    batch_size = 16\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Training')\n",
    "    print('Device:',device)\n",
    "    which_time = 0\n",
    "    N_EPOCHS = 10\n",
    "    HIDDEN_DIM = 300 \n",
    "    OUTPUT_DIM = 3\n",
    "    N_LAYERS = 2\n",
    "    BIDIRECTIONAL = True\n",
    "    DROPOUT = 0.5\n",
    "    \n",
    "    for topic in ['Ivermectin']:\n",
    "        file_name=topic\n",
    "        path = '../Project2/'+topic+'/'\n",
    "\n",
    "        for _target in ['domain']:\n",
    "\n",
    "            if topic=='biden':\n",
    "                ndf=pd.read_csv(path+topic+'_selected.csv')\n",
    "                print(ndf.shape)\n",
    "                ndf.dropna(subset=['retweeted_status_screen_name'],inplace=True)\n",
    "                print(ndf.shape)\n",
    "                m2=52\n",
    "                label1='Democratic'\n",
    "                label2='Republican'\n",
    "\n",
    "            elif topic=='Ivermectin':\n",
    "                ndf=pd.read_csv(path+topic+'_selected_noNA.csv')\n",
    "\n",
    "                m2=241\n",
    "                label1='Misinformation-related'\n",
    "                label2='Mainstream'\n",
    "\n",
    "\n",
    "\n",
    "            ndf['urls']=ndf['urls'].str.split()\n",
    "            ndf=ndf.explode('urls')\n",
    "            ndf['domain']=ndf['urls'].apply(lambda x:  extract_domain(x))\n",
    "            modularity=pd.read_csv(path+file_name+'_networkx.csv')\n",
    "\n",
    "\n",
    "            usertypes=['user_screen_name','retweeted_status_screen_name','user_mentions_screen_names']\n",
    "            rank=0\n",
    "            topn=1000000\n",
    "\n",
    "\n",
    "            modularity=modularity.rename(columns={'Id':'user_screen_name'})\n",
    "\n",
    "\n",
    "\n",
    "            ndf=updatecol(ndf,modularity,'modularity_class','user_screen_name')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            nodes1=ndf[ndf['modularity_class']==38]['user_screen_name'].drop_duplicates().tolist()# Iverlectin related\n",
    "            nodes2=ndf[ndf['modularity_class']==m2]['user_screen_name'].drop_duplicates().tolist()# 241 mainstream related\n",
    "\n",
    "\n",
    "            print('Processing finished!')\n",
    "\n",
    "\n",
    "\n",
    "            ndf0=ndf[ndf['urls']!='']\n",
    "            ndf1=ndf0.groupby(['user_screen_name'])[_target].apply(list).reset_index()\n",
    "\n",
    "            lst=ndf0.groupby(['user_screen_name'])[_target].apply(list).tolist()\n",
    "\n",
    "            def bihashtagGraph(_lst,_i):\n",
    "                _i=0\n",
    "                #terms_bigram = [list(bigrams(tweet)) if len(tweet)>1 else (tweet,'') for tweet in _lst]\n",
    "                terms_bigram = [list(bigrams(tweet)) for tweet in _lst]\n",
    "\n",
    "                d=[x for x in terms_bigram if x!=[]]\n",
    "\n",
    "                lst=[]\n",
    "                for each in d:\n",
    "                    if len(each)==0:\n",
    "                        lst.append(each)\n",
    "                    else:\n",
    "                        [lst.append(x) for x in each]\n",
    "\n",
    "                _hadf=pd.DataFrame(lst)\n",
    "\n",
    "                _K=nx.from_pandas_edgelist(_hadf,0,1)\n",
    "\n",
    "                nx.write_gexf(_K,'../Project2/'+topic+'/'+str(_i)+'_cooccur.gexf')\n",
    "                return _K,_hadf\n",
    "\n",
    "            G,hdf=bihashtagGraph(lst,topic+'_'+_target)\n",
    "\n",
    "            from fastnode2vec import Graph, Node2Vec\n",
    "            def fast_node2vec(_df):\n",
    "                '''\n",
    "                _df:retweetdf,mentiondf\n",
    "                _save:saving folder path\n",
    "                _file:csv name, e.g., vaccine, mask\n",
    "                saving_name: RT or MT\n",
    "                '''\n",
    "\n",
    "                _tuples=_df.to_records(index=False)\n",
    "\n",
    "                _lst=list(_tuples)\n",
    "\n",
    "                _graph = Graph(_lst, directed=False, weighted=False)\n",
    "\n",
    "                _n2v = Node2Vec(_graph, dim=300, walk_length=500, context=10, p=2.0, q=0.5, workers=-1)\n",
    "\n",
    "                _n2v.train(epochs=300)\n",
    "\n",
    "\n",
    "                return _n2v\n",
    "\n",
    "\n",
    "\n",
    "            model=fast_node2vec(hdf)\n",
    "\n",
    "            ranking='random'\n",
    "            #model.wv.save('../Project2/'+topic+'/'+ranking+'_'+_target+'_co_occurrenc_n2v_drop_duplicates.wv')\n",
    "            #model= KeyedVectors.load('../Project2/'+topic+'/'+ranking+'_'+_target+'_co_occurrenc_n2v_drop_duplicates.wv')\n",
    "            model = KeyedVectors.load_word2vec_format('../Project2/'+topic+'/'+topic+'_'+_target+'.bin')\n",
    "            def convert_vec(x):\n",
    "                _n=0\n",
    "                try:\n",
    "                    if len(x)==0:\n",
    "                        return model.get_vector(x[0])\n",
    "                    else:\n",
    "                        _lst= [model.get_vector(y) for y in x]\n",
    "                        _sum=0\n",
    "                        for each in _lst:\n",
    "                            _sum=_sum+each\n",
    "                        return _sum\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    _n+=1\n",
    "                             \n",
    "                    return None\n",
    "\n",
    "\n",
    "\n",
    "            ndf1[_target]=ndf1[_target].apply(lambda x: convert_vec(x))\n",
    "\n",
    "            ndf1=ndf1.rename(columns={_target:'vec'})\n",
    "\n",
    "            ndf1.dropna(subset=['vec'], inplace=True)\n",
    "\n",
    "            ndf1.reset_index(inplace=True,drop=True)\n",
    "\n",
    "            dfv=ndf1['vec']\n",
    "            dfv=pd.DataFrame(dfv)\n",
    "\n",
    "            values=np.array([x[0] for x in dfv.values])\n",
    "\n",
    "            df_values=pd.DataFrame(values)\n",
    "\n",
    "            df_values['user_screen_name']=ndf1['user_screen_name']\n",
    "\n",
    "\n",
    "            data=df_values\n",
    "\n",
    "            data.loc[data['user_screen_name'].isin(nodes1),'labels']=label1\n",
    "\n",
    "            data.loc[data['user_screen_name'].isin(nodes2),'labels']=label2\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweeted_status_screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>urls</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>domain</th>\n",
       "      <th>modularity_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joecorre</td>\n",
       "      <td>2021/01/09</td>\n",
       "      <td>joevaron</td>\n",
       "      <td>rt @joevaron: what&amp;amp;#039;s behind the iverm...</td>\n",
       "      <td>https://www.medpagetoday.com/infectiousdisease...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>medpagetoday.com</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LucianeSerra77</td>\n",
       "      <td>2021/01/09</td>\n",
       "      <td>strYker555</td>\n",
       "      <td>rt @stryker555: @drdavidsamadi ivermectin is p...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/32513289/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nih.gov</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DavidStevenson1</td>\n",
       "      <td>2021/01/09</td>\n",
       "      <td>SaveLivesMplus</td>\n",
       "      <td>rt @savelivesmplus: what's behind the #ivermec...</td>\n",
       "      <td>https://www.medpagetoday.com/infectiousdisease...</td>\n",
       "      <td>Ivermectin</td>\n",
       "      <td>medpagetoday.com</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DavidStevenson1</td>\n",
       "      <td>2021/01/09</td>\n",
       "      <td>DrKellyVictory</td>\n",
       "      <td>rt @drkellyvictory: focus on #covid19 #earlytr...</td>\n",
       "      <td>https://twitter.com/covid19critical/status/134...</td>\n",
       "      <td>COVID19', 'EarlyTreatment', 'Ivermectin', 'hyd...</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PawanSomanchi</td>\n",
       "      <td>2021/01/09</td>\n",
       "      <td>TrialsiteN</td>\n",
       "      <td>rt @trialsiten: ivermectin and covid-19 discus...</td>\n",
       "      <td>https://trialsitenews.com/ivermectin-and-covid...</td>\n",
       "      <td>Ivermectin', 'COVID19', 'healthcare</td>\n",
       "      <td>trialsitenews.com</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26373</th>\n",
       "      <td>wqbelle</td>\n",
       "      <td>2022/02/15</td>\n",
       "      <td>wqbelle</td>\n",
       "      <td>rt @wqbelle: ivermectin: i refuse medical advi...</td>\n",
       "      <td>https://www.williamquincybelle.com/2021/11/ive...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>williamquincybelle.com</td>\n",
       "      <td>882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26374</th>\n",
       "      <td>WSjp_insight</td>\n",
       "      <td>2022/02/15</td>\n",
       "      <td>WSjp_insight</td>\n",
       "      <td>rt @wsjp_insight:  #ivermectin? @us_fda @usato...</td>\n",
       "      <td>http://ws-jp.co.jp/2021/08/14/coronavirus-33/</td>\n",
       "      <td>ivermectin</td>\n",
       "      <td>ws-jp.co.jp</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26375</th>\n",
       "      <td>grawoig</td>\n",
       "      <td>2020/07/08</td>\n",
       "      <td>grawoig</td>\n",
       "      <td>rt @grawoig: article:  (open)\"the fda-approved...</td>\n",
       "      <td>https://lnkd.in/eB8Fhyc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lnkd.in</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26376</th>\n",
       "      <td>grawoig</td>\n",
       "      <td>2020/07/08</td>\n",
       "      <td>grawoig</td>\n",
       "      <td>rt @grawoig: preprint:   (open)\"icon (ivermect...</td>\n",
       "      <td>https://lnkd.in/dpGwPFH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lnkd.in</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26377</th>\n",
       "      <td>madrynha2012</td>\n",
       "      <td>2020/07/08</td>\n",
       "      <td>grawoig</td>\n",
       "      <td>rt @grawoig: article:  (open)\"the fda-approved...</td>\n",
       "      <td>https://lnkd.in/eB8Fhyc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lnkd.in</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26378 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_screen_name  created_at retweeted_status_screen_name  \\\n",
       "0             joecorre  2021/01/09                     joevaron   \n",
       "1       LucianeSerra77  2021/01/09                   strYker555   \n",
       "2      DavidStevenson1  2021/01/09               SaveLivesMplus   \n",
       "3      DavidStevenson1  2021/01/09               DrKellyVictory   \n",
       "4        PawanSomanchi  2021/01/09                   TrialsiteN   \n",
       "...                ...         ...                          ...   \n",
       "26373          wqbelle  2022/02/15                      wqbelle   \n",
       "26374     WSjp_insight  2022/02/15                 WSjp_insight   \n",
       "26375          grawoig  2020/07/08                      grawoig   \n",
       "26376          grawoig  2020/07/08                      grawoig   \n",
       "26377     madrynha2012  2020/07/08                      grawoig   \n",
       "\n",
       "                                                    text  \\\n",
       "0      rt @joevaron: what&amp;#039;s behind the iverm...   \n",
       "1      rt @stryker555: @drdavidsamadi ivermectin is p...   \n",
       "2      rt @savelivesmplus: what's behind the #ivermec...   \n",
       "3      rt @drkellyvictory: focus on #covid19 #earlytr...   \n",
       "4      rt @trialsiten: ivermectin and covid-19 discus...   \n",
       "...                                                  ...   \n",
       "26373  rt @wqbelle: ivermectin: i refuse medical advi...   \n",
       "26374  rt @wsjp_insight:  #ivermectin? @us_fda @usato...   \n",
       "26375  rt @grawoig: article:  (open)\"the fda-approved...   \n",
       "26376  rt @grawoig: preprint:   (open)\"icon (ivermect...   \n",
       "26377  rt @grawoig: article:  (open)\"the fda-approved...   \n",
       "\n",
       "                                                    urls  \\\n",
       "0      https://www.medpagetoday.com/infectiousdisease...   \n",
       "1              https://pubmed.ncbi.nlm.nih.gov/32513289/   \n",
       "2      https://www.medpagetoday.com/infectiousdisease...   \n",
       "3      https://twitter.com/covid19critical/status/134...   \n",
       "4      https://trialsitenews.com/ivermectin-and-covid...   \n",
       "...                                                  ...   \n",
       "26373  https://www.williamquincybelle.com/2021/11/ive...   \n",
       "26374      http://ws-jp.co.jp/2021/08/14/coronavirus-33/   \n",
       "26375                            https://lnkd.in/eB8Fhyc   \n",
       "26376                            https://lnkd.in/dpGwPFH   \n",
       "26377                            https://lnkd.in/eB8Fhyc   \n",
       "\n",
       "                                                hashtags  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2                                             Ivermectin   \n",
       "3      COVID19', 'EarlyTreatment', 'Ivermectin', 'hyd...   \n",
       "4                    Ivermectin', 'COVID19', 'healthcare   \n",
       "...                                                  ...   \n",
       "26373                                                NaN   \n",
       "26374                                         ivermectin   \n",
       "26375                                                NaN   \n",
       "26376                                                NaN   \n",
       "26377                                                NaN   \n",
       "\n",
       "                       domain modularity_class  \n",
       "0            medpagetoday.com               38  \n",
       "1                     nih.gov              732  \n",
       "2            medpagetoday.com               38  \n",
       "3                 twitter.com               38  \n",
       "4           trialsitenews.com               38  \n",
       "...                       ...              ...  \n",
       "26373  williamquincybelle.com              882  \n",
       "26374             ws-jp.co.jp              240  \n",
       "26375                 lnkd.in               38  \n",
       "26376                 lnkd.in               38  \n",
       "26377                 lnkd.in               38  \n",
       "\n",
       "[26378 rows x 8 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004nino</td>\n",
       "      <td>[11alive.com, wapo.st]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0057005</td>\n",
       "      <td>[7news.com.au]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>007Battledress</td>\n",
       "      <td>[frontiersin.org]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>007Haarp</td>\n",
       "      <td>[frontiersin.org]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010Bravo</td>\n",
       "      <td>[zerohedge.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21058</th>\n",
       "      <td>zxcvbn7531</td>\n",
       "      <td>[indiatoday.in]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21059</th>\n",
       "      <td>zxcvbnlkjhgf</td>\n",
       "      <td>[covidcandy.net, indiatoday.in]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21060</th>\n",
       "      <td>zywiecPolska</td>\n",
       "      <td>[covid19criticalcare.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21061</th>\n",
       "      <td>zzcbw</td>\n",
       "      <td>[news24.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21062</th>\n",
       "      <td>zzzdogman</td>\n",
       "      <td>[theburningplatform.com]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21063 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_screen_name                           domain\n",
       "0              004nino           [11alive.com, wapo.st]\n",
       "1              0057005                   [7news.com.au]\n",
       "2       007Battledress                [frontiersin.org]\n",
       "3             007Haarp                [frontiersin.org]\n",
       "4             010Bravo                  [zerohedge.com]\n",
       "...                ...                              ...\n",
       "21058       zxcvbn7531                  [indiatoday.in]\n",
       "21059     zxcvbnlkjhgf  [covidcandy.net, indiatoday.in]\n",
       "21060     zywiecPolska        [covid19criticalcare.com]\n",
       "21061            zzcbw                     [news24.com]\n",
       "21062        zzzdogman         [theburningplatform.com]\n",
       "\n",
       "[21063 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf0.groupby(['user_screen_name'])[_target].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004nino</td>\n",
       "      <td>[-1.1925892, -1.0352516, 4.4351883, -0.8522906...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0057005</td>\n",
       "      <td>[3.9160743, 0.1638995, 2.231088, 0.5390191, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>007Battledress</td>\n",
       "      <td>[1.2187577, 0.761269, -0.29233846, -0.36979178...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>007Haarp</td>\n",
       "      <td>[1.2187577, 0.761269, -0.29233846, -0.36979178...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010Bravo</td>\n",
       "      <td>[0.95581913, -0.9581492, 1.5755782, 1.6082416,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20538</th>\n",
       "      <td>zwarteaap</td>\n",
       "      <td>[2.1852036, 0.00435567, -1.193927, -3.5442545,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20539</th>\n",
       "      <td>zxcvbn7531</td>\n",
       "      <td>[-0.28646815, 1.7624307, -1.632462, 1.0369163,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20540</th>\n",
       "      <td>zxcvbnlkjhgf</td>\n",
       "      <td>[-1.6408136, 0.92541635, -2.1548944, 2.7156286...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20541</th>\n",
       "      <td>zywiecPolska</td>\n",
       "      <td>[0.18053854, -0.51028895, 1.8172691, 1.2114838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20542</th>\n",
       "      <td>zzcbw</td>\n",
       "      <td>[0.49207762, -1.1066989, 2.3009372, -1.737632,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20543 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_screen_name                                                vec\n",
       "0              004nino  [-1.1925892, -1.0352516, 4.4351883, -0.8522906...\n",
       "1              0057005  [3.9160743, 0.1638995, 2.231088, 0.5390191, 1....\n",
       "2       007Battledress  [1.2187577, 0.761269, -0.29233846, -0.36979178...\n",
       "3             007Haarp  [1.2187577, 0.761269, -0.29233846, -0.36979178...\n",
       "4             010Bravo  [0.95581913, -0.9581492, 1.5755782, 1.6082416,...\n",
       "...                ...                                                ...\n",
       "20538        zwarteaap  [2.1852036, 0.00435567, -1.193927, -3.5442545,...\n",
       "20539       zxcvbn7531  [-0.28646815, 1.7624307, -1.632462, 1.0369163,...\n",
       "20540     zxcvbnlkjhgf  [-1.6408136, 0.92541635, -2.1548944, 2.7156286...\n",
       "20541     zywiecPolska  [0.18053854, -0.51028895, 1.8172691, 1.2114838...\n",
       "20542            zzcbw  [0.49207762, -1.1066989, 2.3009372, -1.737632,...\n",
       "\n",
       "[20543 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.192589</td>\n",
       "      <td>-1.035252</td>\n",
       "      <td>4.435188</td>\n",
       "      <td>-0.852291</td>\n",
       "      <td>-1.301320</td>\n",
       "      <td>0.329301</td>\n",
       "      <td>2.453095</td>\n",
       "      <td>-1.689740</td>\n",
       "      <td>-0.370342</td>\n",
       "      <td>0.654340</td>\n",
       "      <td>...</td>\n",
       "      <td>3.770248</td>\n",
       "      <td>3.787398</td>\n",
       "      <td>1.353218</td>\n",
       "      <td>-2.120125</td>\n",
       "      <td>1.107438</td>\n",
       "      <td>-0.180127</td>\n",
       "      <td>-0.503752</td>\n",
       "      <td>0.059031</td>\n",
       "      <td>004nino</td>\n",
       "      <td>Mainstream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.916074</td>\n",
       "      <td>0.163899</td>\n",
       "      <td>2.231088</td>\n",
       "      <td>0.539019</td>\n",
       "      <td>1.506174</td>\n",
       "      <td>0.472400</td>\n",
       "      <td>-0.110427</td>\n",
       "      <td>-0.144782</td>\n",
       "      <td>0.017744</td>\n",
       "      <td>3.252798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913391</td>\n",
       "      <td>3.462751</td>\n",
       "      <td>0.568594</td>\n",
       "      <td>3.323148</td>\n",
       "      <td>1.274090</td>\n",
       "      <td>-2.430989</td>\n",
       "      <td>0.604043</td>\n",
       "      <td>0.170677</td>\n",
       "      <td>0057005</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.218758</td>\n",
       "      <td>0.761269</td>\n",
       "      <td>-0.292338</td>\n",
       "      <td>-0.369792</td>\n",
       "      <td>2.921480</td>\n",
       "      <td>1.366497</td>\n",
       "      <td>-0.489403</td>\n",
       "      <td>-1.534574</td>\n",
       "      <td>-2.812563</td>\n",
       "      <td>-0.703177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516682</td>\n",
       "      <td>-2.161208</td>\n",
       "      <td>-0.346731</td>\n",
       "      <td>0.663106</td>\n",
       "      <td>-0.927841</td>\n",
       "      <td>-1.498977</td>\n",
       "      <td>0.794254</td>\n",
       "      <td>-1.655879</td>\n",
       "      <td>007Battledress</td>\n",
       "      <td>Misinformation-related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.218758</td>\n",
       "      <td>0.761269</td>\n",
       "      <td>-0.292338</td>\n",
       "      <td>-0.369792</td>\n",
       "      <td>2.921480</td>\n",
       "      <td>1.366497</td>\n",
       "      <td>-0.489403</td>\n",
       "      <td>-1.534574</td>\n",
       "      <td>-2.812563</td>\n",
       "      <td>-0.703177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516682</td>\n",
       "      <td>-2.161208</td>\n",
       "      <td>-0.346731</td>\n",
       "      <td>0.663106</td>\n",
       "      <td>-0.927841</td>\n",
       "      <td>-1.498977</td>\n",
       "      <td>0.794254</td>\n",
       "      <td>-1.655879</td>\n",
       "      <td>007Haarp</td>\n",
       "      <td>Misinformation-related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.955819</td>\n",
       "      <td>-0.958149</td>\n",
       "      <td>1.575578</td>\n",
       "      <td>1.608242</td>\n",
       "      <td>1.817987</td>\n",
       "      <td>-2.801309</td>\n",
       "      <td>-0.717024</td>\n",
       "      <td>2.961999</td>\n",
       "      <td>-0.553000</td>\n",
       "      <td>-1.615809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216807</td>\n",
       "      <td>0.057576</td>\n",
       "      <td>0.746806</td>\n",
       "      <td>-0.121231</td>\n",
       "      <td>-3.023936</td>\n",
       "      <td>0.385342</td>\n",
       "      <td>-1.125083</td>\n",
       "      <td>-0.440501</td>\n",
       "      <td>010Bravo</td>\n",
       "      <td>Misinformation-related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20538</th>\n",
       "      <td>2.185204</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>-1.193927</td>\n",
       "      <td>-3.544255</td>\n",
       "      <td>-0.642606</td>\n",
       "      <td>-0.745987</td>\n",
       "      <td>-0.597001</td>\n",
       "      <td>-0.650633</td>\n",
       "      <td>1.492831</td>\n",
       "      <td>0.892360</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.056200</td>\n",
       "      <td>-0.725686</td>\n",
       "      <td>2.722271</td>\n",
       "      <td>1.222054</td>\n",
       "      <td>-0.573654</td>\n",
       "      <td>2.767612</td>\n",
       "      <td>2.526883</td>\n",
       "      <td>-1.675663</td>\n",
       "      <td>zwarteaap</td>\n",
       "      <td>Misinformation-related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20539</th>\n",
       "      <td>-0.286468</td>\n",
       "      <td>1.762431</td>\n",
       "      <td>-1.632462</td>\n",
       "      <td>1.036916</td>\n",
       "      <td>2.024808</td>\n",
       "      <td>1.289054</td>\n",
       "      <td>0.556149</td>\n",
       "      <td>-0.295394</td>\n",
       "      <td>1.839689</td>\n",
       "      <td>-0.466775</td>\n",
       "      <td>...</td>\n",
       "      <td>4.257350</td>\n",
       "      <td>0.111323</td>\n",
       "      <td>1.849908</td>\n",
       "      <td>-0.999748</td>\n",
       "      <td>1.278894</td>\n",
       "      <td>-1.523608</td>\n",
       "      <td>0.745518</td>\n",
       "      <td>-3.417775</td>\n",
       "      <td>zxcvbn7531</td>\n",
       "      <td>Misinformation-related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20540</th>\n",
       "      <td>-1.640814</td>\n",
       "      <td>0.925416</td>\n",
       "      <td>-2.154894</td>\n",
       "      <td>2.715629</td>\n",
       "      <td>1.743415</td>\n",
       "      <td>2.676794</td>\n",
       "      <td>-2.096154</td>\n",
       "      <td>-0.189973</td>\n",
       "      <td>3.088216</td>\n",
       "      <td>1.172794</td>\n",
       "      <td>...</td>\n",
       "      <td>4.637537</td>\n",
       "      <td>-1.217018</td>\n",
       "      <td>1.025057</td>\n",
       "      <td>-2.704226</td>\n",
       "      <td>1.858948</td>\n",
       "      <td>-4.097062</td>\n",
       "      <td>1.937569</td>\n",
       "      <td>-3.069698</td>\n",
       "      <td>zxcvbnlkjhgf</td>\n",
       "      <td>Misinformation-related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20541</th>\n",
       "      <td>0.180539</td>\n",
       "      <td>-0.510289</td>\n",
       "      <td>1.817269</td>\n",
       "      <td>1.211484</td>\n",
       "      <td>0.669083</td>\n",
       "      <td>2.385235</td>\n",
       "      <td>0.401899</td>\n",
       "      <td>-0.729362</td>\n",
       "      <td>-0.270162</td>\n",
       "      <td>2.208564</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.014079</td>\n",
       "      <td>-0.834520</td>\n",
       "      <td>0.455198</td>\n",
       "      <td>-0.700396</td>\n",
       "      <td>-0.628340</td>\n",
       "      <td>1.977684</td>\n",
       "      <td>-0.768917</td>\n",
       "      <td>3.163119</td>\n",
       "      <td>zywiecPolska</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20542</th>\n",
       "      <td>0.492078</td>\n",
       "      <td>-1.106699</td>\n",
       "      <td>2.300937</td>\n",
       "      <td>-1.737632</td>\n",
       "      <td>-0.362814</td>\n",
       "      <td>-0.755075</td>\n",
       "      <td>1.960143</td>\n",
       "      <td>1.406003</td>\n",
       "      <td>-0.912616</td>\n",
       "      <td>1.262503</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.649727</td>\n",
       "      <td>-2.659510</td>\n",
       "      <td>-0.674642</td>\n",
       "      <td>-0.086148</td>\n",
       "      <td>1.639723</td>\n",
       "      <td>0.906467</td>\n",
       "      <td>1.338985</td>\n",
       "      <td>-0.193903</td>\n",
       "      <td>zzcbw</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20543 rows × 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -1.192589 -1.035252  4.435188 -0.852291 -1.301320  0.329301  2.453095   \n",
       "1      3.916074  0.163899  2.231088  0.539019  1.506174  0.472400 -0.110427   \n",
       "2      1.218758  0.761269 -0.292338 -0.369792  2.921480  1.366497 -0.489403   \n",
       "3      1.218758  0.761269 -0.292338 -0.369792  2.921480  1.366497 -0.489403   \n",
       "4      0.955819 -0.958149  1.575578  1.608242  1.817987 -2.801309 -0.717024   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "20538  2.185204  0.004356 -1.193927 -3.544255 -0.642606 -0.745987 -0.597001   \n",
       "20539 -0.286468  1.762431 -1.632462  1.036916  2.024808  1.289054  0.556149   \n",
       "20540 -1.640814  0.925416 -2.154894  2.715629  1.743415  2.676794 -2.096154   \n",
       "20541  0.180539 -0.510289  1.817269  1.211484  0.669083  2.385235  0.401899   \n",
       "20542  0.492078 -1.106699  2.300937 -1.737632 -0.362814 -0.755075  1.960143   \n",
       "\n",
       "              7         8         9  ...       292       293       294  \\\n",
       "0     -1.689740 -0.370342  0.654340  ...  3.770248  3.787398  1.353218   \n",
       "1     -0.144782  0.017744  3.252798  ...  0.913391  3.462751  0.568594   \n",
       "2     -1.534574 -2.812563 -0.703177  ...  0.516682 -2.161208 -0.346731   \n",
       "3     -1.534574 -2.812563 -0.703177  ...  0.516682 -2.161208 -0.346731   \n",
       "4      2.961999 -0.553000 -1.615809  ... -0.216807  0.057576  0.746806   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "20538 -0.650633  1.492831  0.892360  ... -2.056200 -0.725686  2.722271   \n",
       "20539 -0.295394  1.839689 -0.466775  ...  4.257350  0.111323  1.849908   \n",
       "20540 -0.189973  3.088216  1.172794  ...  4.637537 -1.217018  1.025057   \n",
       "20541 -0.729362 -0.270162  2.208564  ... -1.014079 -0.834520  0.455198   \n",
       "20542  1.406003 -0.912616  1.262503  ... -0.649727 -2.659510 -0.674642   \n",
       "\n",
       "            295       296       297       298       299  user_screen_name  \\\n",
       "0     -2.120125  1.107438 -0.180127 -0.503752  0.059031           004nino   \n",
       "1      3.323148  1.274090 -2.430989  0.604043  0.170677           0057005   \n",
       "2      0.663106 -0.927841 -1.498977  0.794254 -1.655879    007Battledress   \n",
       "3      0.663106 -0.927841 -1.498977  0.794254 -1.655879          007Haarp   \n",
       "4     -0.121231 -3.023936  0.385342 -1.125083 -0.440501          010Bravo   \n",
       "...         ...       ...       ...       ...       ...               ...   \n",
       "20538  1.222054 -0.573654  2.767612  2.526883 -1.675663         zwarteaap   \n",
       "20539 -0.999748  1.278894 -1.523608  0.745518 -3.417775        zxcvbn7531   \n",
       "20540 -2.704226  1.858948 -4.097062  1.937569 -3.069698      zxcvbnlkjhgf   \n",
       "20541 -0.700396 -0.628340  1.977684 -0.768917  3.163119      zywiecPolska   \n",
       "20542 -0.086148  1.639723  0.906467  1.338985 -0.193903             zzcbw   \n",
       "\n",
       "                       labels  \n",
       "0                  Mainstream  \n",
       "1                         NaN  \n",
       "2      Misinformation-related  \n",
       "3      Misinformation-related  \n",
       "4      Misinformation-related  \n",
       "...                       ...  \n",
       "20538  Misinformation-related  \n",
       "20539  Misinformation-related  \n",
       "20540  Misinformation-related  \n",
       "20541                     NaN  \n",
       "20542                     NaN  \n",
       "\n",
       "[20543 rows x 302 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading graph: 100%|██████████| 5315/5315 [00:00<00:00, 263744.33it/s]\n",
      "Training:   1%|          | 51304/5020000 [00:00<00:42, 115652.04it/s]"
     ]
    }
   ],
   "source": [
    "from fastnode2vec import Graph, Node2Vec\n",
    "def fast_node2vec(_df):\n",
    "                '''\n",
    "                _df:retweetdf,mentiondf\n",
    "                _save:saving folder path\n",
    "                _file:csv name, e.g., vaccine, mask\n",
    "                saving_name: RT or MT\n",
    "                '''\n",
    "\n",
    "                _tuples=_df.to_records(index=False)\n",
    "\n",
    "                _lst=list(_tuples)\n",
    "\n",
    "                _graph = Graph(_lst, directed=False, weighted=False)\n",
    "\n",
    "                _n2v = Node2Vec(_graph, dim=300, walk_length=500, context=10, p=2.0, q=0.5, workers=-1)\n",
    "\n",
    "                _n2v.train(epochs=10000)\n",
    "\n",
    "\n",
    "                return _n2v\n",
    "\n",
    "\n",
    "\n",
    "model1=fast_node2vec(hdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nodevectors import Node2Vec\n",
    "g2v = Node2Vec(\n",
    "    n_components=300,\n",
    "    walklen=500,\n",
    "    threads=0,\n",
    "    return_weight=2.0,\n",
    "    neighbor_weight=0.5,\n",
    "    epochs=300,\n",
    "    \n",
    "    w2vparams={'min_count':1},\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making walks... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lawrencexu/Library/Python/3.7/lib/python/site-packages/numba/np/ufunc/parallel.py:366: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2021 update 1 or later i.e., TBB_INTERFACE_VERSION >= 12010. Found TBB_INTERFACE_VERSION = 11000. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, T=1854.10\n",
      "Mapping Walk Names... Done, T=44.92\n",
      "Training W2V... WARNING: gensim word2vec version is unoptimizedTry version 3.6 if on windows, versions 3.7 and 3.8 have had issues\n",
      "Done, T=211.52\n"
     ]
    }
   ],
   "source": [
    "g2v.fit(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502\n",
      "(502, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "g2v.save_vectors('../Project2/'+topic+'/'+topic+'_'+_target+'.bin')\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('../Project2/'+topic+'/'+topic+'_'+_target+'.bin')\n",
    "\n",
    "print(len(model.index2entity))\n",
    "print(model.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ww.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20543, 302)\n"
     ]
    }
   ],
   "source": [
    "            print(data.shape)\n",
    "\n",
    "            data.drop_duplicates(subset=list(range(300)),inplace=True)\n",
    "\n",
    "            data.dropna(subset=['labels'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Device: cpu\n",
      "Processing finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading graph: 100%|██████████| 5315/5315 [00:00<00:00, 266296.27it/s]\n",
      "Training:  90%|████████▉ | 135008/150600 [00:00<00:00, 200335.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Key 'suntimes.com' not present\"\n",
      "\"Key 'nakedcapitalism.com' not present\"\n",
      "\"Key 'drugtargetreview.com' not present\"\n",
      "\"Key 'syrianews.cc' not present\"\n",
      "\"Key 'brisbanetimes.com.au' not present\"\n",
      "\"Key 'sciencemag.org' not present\"\n",
      "\"Key 'anchoragepress.com' not present\"\n",
      "\"Key 'wsws.org' not present\"\n",
      "\"Key 'pharmacypracticenews.com' not present\"\n",
      "\"Key 'taiwannews.com.tw' not present\"\n",
      "\"Key 'alethonews.com' not present\"\n",
      "\"Key 'odishatv.in' not present\"\n",
      "\"Key 'pfizer.com' not present\"\n",
      "\"Key 'nyoooz.com' not present\"\n",
      "\"Key 'firstcoastnews.com' not present\"\n",
      "\"Key 'amerexperience.com' not present\"\n",
      "\"Key 'upi.com' not present\"\n",
      "\"Key 'science.news' not present\"\n",
      "\"Key 'politicsweb.co.za' not present\"\n",
      "\"Key 'iltalehti.fi' not present\"\n",
      "\"Key 'suntimes.com' not present\"\n",
      "\"Key 'blabber.buzz' not present\"\n",
      "\"Key 'us.org' not present\"\n",
      "\"Key 'sciencedaily.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'etcnda.com' not present\"\n",
      "\"Key 'trendnews.eu' not present\"\n",
      "\"Key 'economictimes.com' not present\"\n",
      "\"Key 'mississippifreepress.org' not present\"\n",
      "\"Key 'whio.com' not present\"\n",
      "\"Key 'australiannationalreview.com' not present\"\n",
      "\"Key 'taiwannews.com.tw' not present\"\n",
      "\"Key 'lifesitenews.com' not present\"\n",
      "\"Key '2gtx.com' not present\"\n",
      "\"Key 'al.com' not present\"\n",
      "\"Key 'boston25news.com' not present\"\n",
      "\"Key 'taiwannews.com.tw' not present\"\n",
      "\"Key 'newstatesman.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'newsbreak.com' not present\"\n",
      "\"Key 'newstatesman.com' not present\"\n",
      "\"Key 'bluebloodz.com' not present\"\n",
      "\"Key 'bbc.in' not present\"\n",
      "\"Key 'whio.com' not present\"\n",
      "\"Key 'anchoragepress.com' not present\"\n",
      "\"Key 'news18.com' not present\"\n",
      "\"Key 'dearpandemic.org' not present\"\n",
      "\"Key 'outbreaknewstoday.com' not present\"\n",
      "\"Key 'unitaid.org' not present\"\n",
      "\"Key 'suntimes.com' not present\"\n",
      "\"Key 'asianetnews.com' not present\"\n",
      "\"Key 'instagram.com' not present\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 150600/150600 [00:00<00:00, 197756.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Key 'vancouversun.com' not present\"\n",
      "\"Key 'buzzsprout.com' not present\"\n",
      "\"Key 'whio.com' not present\"\n",
      "\"Key 'fox61.com' not present\"\n",
      "\"Key 'clevelandclinic.org' not present\"\n",
      "\"Key 'wreg.com' not present\"\n",
      "\"Key '2gtx.com' not present\"\n",
      "\"Key 'newschannel5.com' not present\"\n",
      "\"Key 'whio.com' not present\"\n",
      "\"Key 'wsws.org' not present\"\n",
      "\"Key 'sun-sentinel.com' not present\"\n",
      "\"Key 'theconversation.com' not present\"\n",
      "\"Key 'joemygod.com' not present\"\n",
      "\"Key 'wtop.com' not present\"\n",
      "\"Key '10tv.com' not present\"\n",
      "\"Key 'newsday.co.tt' not present\"\n",
      "\"Key 'eutimes.net' not present\"\n",
      "\"Key 'wtop.com' not present\"\n",
      "\"Key 'thephaser.com' not present\"\n",
      "\"Key 'metro.co.uk' not present\"\n",
      "\"Key 'wcnc.com' not present\"\n",
      "\"Key 'wsws.org' not present\"\n",
      "\"Key 'wsws.org' not present\"\n",
      "\"Key 'iafrica24.com' not present\"\n",
      "\"Key 'rumormillnews.com' not present\"\n",
      "\"Key 'instagram.com' not present\"\n",
      "\"Key 'businesstoday.in' not present\"\n",
      "\"Key 'kvoa.com' not present\"\n",
      "\"Key 'leacountytribune.com' not present\"\n",
      "\"Key 'chwpeds.com' not present\"\n",
      "\"Key 'afinalwarning.com' not present\"\n",
      "\"Key 'rclutz.com' not present\"\n",
      "\"Key 'realityteam.org' not present\"\n",
      "\"Key 'clarin.com' not present\"\n",
      "\"Key 'wentworthreport.com' not present\"\n",
      "\"Key 'archive.org' not present\"\n",
      "\"Key 'whio.com' not present\"\n",
      "\"Key 'lethbridgenewsnow.com' not present\"\n",
      "\"Key 'lohud.com' not present\"\n",
      "\"Key 'menshealth.com' not present\"\n",
      "\"Key 'sci-hub.tw' not present\"\n",
      "\"Key 'sci-hub.tw' not present\"\n",
      "\"Key 'myfreedoctor.com' not present\"\n",
      "\"Key 'canadafreepress.com' not present\"\n",
      "\"Key 'sciencedaily.com' not present\"\n",
      "\"Key 'rumormillnews.com' not present\"\n",
      "\"Key 'farmersweekly.co.za' not present\"\n",
      "\"Key 'instagram.com' not present\"\n",
      "\"Key 'reclaimthenet.org' not present\"\n",
      "\"Key 'myCatholicdoctor.com' not present\"\n",
      "\"Key 'sciencemag.org' not present\"\n",
      "\"Key 'medicalnewstoday.com' not present\"\n",
      "\"Key 'metro.co.uk' not present\"\n",
      "\"Key 'science.news' not present\"\n",
      "\"Key 'kshb.com' not present\"\n",
      "\"Key 'firstcoastnews.com' not present\"\n",
      "\"Key 'yle.fi' not present\"\n",
      "\"Key 'oregonstate.edu' not present\"\n",
      "\"Key 'nanoappsmedical.com' not present\"\n",
      "\"Key 'anchor.fm' not present\"\n",
      "\"Key 'nakedcapitalism.com' not present\"\n",
      "\"Key 'wsws.org' not present\"\n",
      "\"Key 'suntimes.com' not present\"\n",
      "\"Key 'economist.com' not present\"\n",
      "\"Key 'theweek.com' not present\"\n",
      "\"Key 'cbn.com' not present\"\n",
      "\"Key 'afinalwarning.com' not present\"\n",
      "\"Key 'taiwannews.com.tw' not present\"\n",
      "\"Key 'cleveland.com' not present\"\n",
      "\"Key 'antiviral.store' not present\"\n",
      "\"Key 'antiviral.store' not present\"\n",
      "\"Key 'antiviral.store' not present\"\n",
      "\"Key 'mississippifreepress.org' not present\"\n",
      "\"Key 'shawlocal.com' not present\"\n",
      "\"Key 'wosu.pm' not present\"\n",
      "\"Key 'medicallyspeaking.in' not present\"\n",
      "\"Key 'chemistryworld.com' not present\"\n",
      "\"Key 'suntimes.com' not present\"\n",
      "\"Key 'economictimes.com' not present\"\n",
      "\"Key 'avaaz.org' not present\"\n",
      "\"Key 'sciencemag.org' not present\"\n",
      "\"Key 'hillreporter.com' not present\"\n",
      "\"Key 'ation-victimes-coronavirus-france.org' not present\"\n",
      "\"Key 'amazon.com' not present\"\n",
      "\"Key 'dailytelegraph.com.au' not present\"\n",
      "\"Key 'ampproject.org' not present\"\n",
      "\"Key 'politicsweb.co.za' not present\"\n",
      "\"Key 'anchoragepress.com' not present\"\n",
      "\"Key 'sj-r.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'theedgemarkets.com' not present\"\n",
      "\"Key 'stuff.co.nz' not present\"\n",
      "\"Key 'sj-r.com' not present\"\n",
      "\"Key 'insider.com' not present\"\n",
      "\"Key 'wncn.tv' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'lethbridgenewsnow.com' not present\"\n",
      "\"Key 'metro.co.uk' not present\"\n",
      "\"Key 'idse.net' not present\"\n",
      "\"Key 'afinalwarning.com' not present\"\n",
      "\"Key 'perthnow.com.au' not present\"\n",
      "\"Key 'wltx.com' not present\"\n",
      "\"Key 'whio.com' not present\"\n",
      "\"Key 'eastmojo.com' not present\"\n",
      "\"Key 'cbn.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'freedomsphoenix.com' not present\"\n",
      "\"Key 'as.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'nine.com.au' not present\"\n",
      "\"Key 'wsws.org' not present\"\n",
      "\"Key 'huffpost.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'flip.it' not present\"\n",
      "\"Key 'sciencemag.org' not present\"\n",
      "\"Key 'lifeboat.com' not present\"\n",
      "\"Key 'wsws.org' not present\"\n",
      "\"Key 'cbs42.com' not present\"\n",
      "\"Key 'newschannel5.com' not present\"\n",
      "\"Key 'wsws.org' not present\"\n",
      "\"Key 'health.gov.au' not present\"\n",
      "\"Key 'eurweb.com' not present\"\n",
      "\"Key 'yourbias.is' not present\"\n",
      "\"Key 'rumormillnews.com' not present\"\n",
      "\"Key 'toddstarnes.com' not present\"\n",
      "\"Key 'wosu.pm' not present\"\n",
      "\"Key 'flip.it' not present\"\n",
      "\"Key 'sciencemag.org' not present\"\n",
      "\"Key 'childrenshealthdefense.org' not present\"\n",
      "\"Key 'cleveland.com' not present\"\n",
      "\"Key 'leacountytribune.com' not present\"\n",
      "\"Key 'abplive.com' not present\"\n",
      "\"Key 'firstcoastnews.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'instagram.com' not present\"\n",
      "\"Key 'wkbw.com' not present\"\n",
      "\"Key 'rumormillnews.com' not present\"\n",
      "\"Key 'sanfordhealth.org' not present\"\n",
      "\"Key 'wwltv.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'thequint.com' not present\"\n",
      "\"Key 'finalcall.com' not present\"\n",
      "\"Key 'uol.com.br' not present\"\n",
      "\"Key 'patentoffice.ir' not present\"\n",
      "\"Key 'theconversation.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'pharmacypracticenews.com' not present\"\n",
      "\"Key 'genengnews.com' not present\"\n",
      "\"Key 'wtop.com' not present\"\n",
      "\"Key 'nakedcapitalism.com' not present\"\n",
      "\"Key 'right.bz' not present\"\n",
      "\"Key 'citybeat.com' not present\"\n",
      "\"Key 'statnews.com' not present\"\n",
      "\"Key 'medicalnewstoday.com' not present\"\n",
      "\"Key 'wftv.com' not present\"\n",
      "\"Key 'gov.scot' not present\"\n",
      "\"Key 'archive.org' not present\"\n",
      "\"Key 'cnbctv18.com' not present\"\n",
      "\"Key 'nice.org.uk' not present\"\n",
      "\"Key '891maxfm.ca' not present\"\n",
      "\"Key 'reclaimthenet.org' not present\"\n",
      "\"Key 'kbs.co.kr' not present\"\n",
      "\"Key 'lifeboat.com' not present\"\n",
      "\"Key 'farmersweekly.co.za' not present\"\n",
      "\"Key 'news4sanantonio.com' not present\"\n",
      "\"Key 'ibtimes.co.in' not present\"\n",
      "\"Key 'miragenews.com' not present\"\n",
      "\"Key 'cleveland.com' not present\"\n",
      "\"Key 'krem.com' not present\"\n",
      "\"Key 'anchor.fm' not present\"\n",
      "\"Key 'taiwannews.com.tw' not present\"\n",
      "\"Key 'suntimes.com' not present\"\n",
      "\"Key 'realityteam.org' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'entityart.co.uk' not present\"\n",
      "\"Key 'pharmacyonair.com' not present\"\n",
      "\"Key 'heavy.com' not present\"\n",
      "\"Key 'ivermectinforcovid.store' not present\"\n",
      "\"Key 'threadreaderapp.com' not present\"\n",
      "\"Key 'cbn.com' not present\"\n",
      "\"Key 'archive.org' not present\"\n",
      "\"Key 'cnb.cx' not present\"\n",
      "\"Key 'pennlive.com' not present\"\n",
      "\"Key 'wtop.com' not present\"\n",
      "\"Key 'bvsalud.org' not present\"\n",
      "\"Key 'suntimes.com' not present\"\n",
      "\"Key 'buzzsprout.com' not present\"\n",
      "\"Key 'nakedcapitalism.com' not present\"\n",
      "\"Key 'wkbw.com' not present\"\n",
      "\"Key 'clarin.com' not present\"\n",
      "\"Key 'shr.lc' not present\"\n",
      "\"Key 'covid19india.org' not present\"\n",
      "\"Key 'haaretz.com' not present\"\n",
      "\"Key 'theglobeandmail.com' not present\"\n",
      "\"Key 'basedunderground.com' not present\"\n",
      "\"Key 'wtop.com' not present\"\n",
      "\"Key 'newshub.lk' not present\"\n",
      "\"Key 'perthnow.com.au' not present\"\n",
      "\"Key 'health.gov.au' not present\"\n",
      "\"Key 'newstarget.com' not present\"\n",
      "\"Key '20minutos.es' not present\"\n",
      "\"Key 'kshb.com' not present\"\n",
      "\"Key 'ksdk.com' not present\"\n",
      "\"Key 'krem.com' not present\"\n",
      "\"Key 'nanoappsmedical.com' not present\"\n",
      "\"Key 'odishatv.in' not present\"\n",
      "\"Key 'insider.com' not present\"\n",
      "\"Key '13wham.com' not present\"\n",
      "\"Key 'suntimes.com' not present\"\n",
      "\"Key 'proboards.com' not present\"\n",
      "\"Key 'sciencemag.org' not present\"\n",
      "\"Key 'firstcoastnews.com' not present\"\n",
      "\"Key 'reseauinternational.net' not present\"\n",
      "\"Key 'suntimes.com' not present\"\n",
      "\"Key 'economist.com' not present\"\n",
      "\"Key 'sj-r.com' not present\"\n",
      "\"Key 'basedunderground.com' not present\"\n",
      "\"Key 'StudyFinds.org' not present\"\n",
      "\"Key 'wtop.com' not present\"\n",
      "\"Key 'reclaimthenet.org' not present\"\n",
      "\"Key 'sanyonews.jp' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'entityart.co.uk' not present\"\n",
      "\"Key '2gtx.com' not present\"\n",
      "\"Key 'thequint.com' not present\"\n",
      "\"Key 'cjonline.com' not present\"\n",
      "\"Key 'themuslimtimes.info' not present\"\n",
      "\"Key 'wgrz.com' not present\"\n",
      "\"Key 'irishtimes.com' not present\"\n",
      "\"Key 'medicalnewstoday.com' not present\"\n",
      "\"Key 'whatsapp.com' not present\"\n",
      "\"Key 'cbs42.com' not present\"\n",
      "\"Key 'daily-sun.com' not present\"\n",
      "\"Key 'news4sanantonio.com' not present\"\n",
      "\"Key 'businesstoday.in' not present\"\n",
      "\"Key 'lcsun-news.com' not present\"\n",
      "\"Key 'theweek.com' not present\"\n",
      "\"Key 'firstpost.com' not present\"\n",
      "\"Key 'lohud.com' not present\"\n",
      "\"Key 'nakedcapitalism.com' not present\"\n",
      "\"Key 'thelancet.com' not present\"\n",
      "\"Key 'statnews.com' not present\"\n",
      "\"Key 'stuff.co.nz' not present\"\n",
      "\"Key '2gtx.com' not present\"\n",
      "\"Key 'drugtargetreview.com' not present\"\n",
      "\"Key 'newstarget.com' not present\"\n",
      "\"Key 'yourtango.com' not present\"\n",
      "\"Key 'yourworldevents.com' not present\"\n",
      "\"Key 'us.org' not present\"\n",
      "\"Key '2gtx.com' not present\"\n",
      "\"Key 'wftv.com' not present\"\n",
      "\"Key 'sciencemag.org' not present\"\n",
      "\"Key 'sciencemag.org' not present\"\n",
      "\"Key 'wkbw.com' not present\"\n",
      "\"Key 'theedgemarkets.com' not present\"\n",
      "\"Key 'whio.com' not present\"\n",
      "\"Key 'clinicaltrialsarena.com' not present\"\n",
      "\"Key 'lifeboat.com' not present\"\n",
      "\"Key 'nbc6.com' not present\"\n",
      "\"Key 'lcsun-news.com' not present\"\n",
      "\"Key 'sciencedaily.com' not present\"\n",
      "\"Key 'wfp.to' not present\"\n",
      "\"Key 'amazon.com' not present\"\n",
      "\"Key 'bbc.in' not present\"\n",
      "\"Key 'politicsweb.co.za' not present\"\n",
      "\"Key 'lifeboat.com' not present\"\n",
      "\"Key 'wikipedia.org' not present\"\n",
      "\"Key 'businesslive.co.za' not present\"\n",
      "\"Key 'nakedcapitalism.com' not present\"\n",
      "\"Key 'economictimes.com' not present\"\n",
      "\"Key 'toddstarnes.com' not present\"\n",
      "\"Key 'flip.it' not present\"\n",
      "\"Key 'macon.com' not present\"\n",
      "\"Key 'businesstoday.in' not present\"\n",
      "\"Key 'ampproject.org' not present\"\n",
      "\"Key 'archive.org' not present\"\n",
      "\"Key 'meaww.com' not present\"\n",
      "\"Key 'wsws.org' not present\"\n",
      "\"Key 'enca.com' not present\"\n",
      "\"Key 'mol.im' not present\"\n",
      "\"Key 'statnews.com' not present\"\n",
      "\"Key 'eastmojo.com' not present\"\n",
      "\"Key 'wosu.pm' not present\"\n",
      "\"Key 'anchor.fm' not present\"\n",
      "\"Key 'wsws.org' not present\"\n",
      "\"Key 'cbsloc.al' not present\"\n",
      "\"Key 'wirenews.org' not present\"\n",
      "\"Key 'jesuismalade.org' not present\"\n",
      "\"Key 'threader.app' not present\"\n",
      "\"Key 'nanoappsmedical.com' not present\"\n",
      "\"Key 'lbry.tv' not present\"\n",
      "\"Key 'meaww.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'anchoragepress.com' not present\"\n",
      "\"Key 'docdroid.net' not present\"\n",
      "\"Key '20minutes.fr' not present\"\n",
      "\"Key 'sfgate.com' not present\"\n",
      "\"Key 'citybeat.com' not present\"\n",
      "\"Key 'koin.com' not present\"\n",
      "\"Key 'lethbridgenewsnow.com' not present\"\n",
      "\"Key 'antiviral.store' not present\"\n",
      "\"Key 'wbay.com' not present\"\n",
      "\"Key 'shawlocal.com' not present\"\n",
      "\"Key 'theweek.com' not present\"\n",
      "\"Key 'livescience.com' not present\"\n",
      "\"Key 'ktsm.com' not present\"\n",
      "\"Key 'sciencemag.org' not present\"\n",
      "\"Key 'theburningplatform.com' not present\"\n",
      "\"Key 'wltx.com' not present\"\n",
      "\"Key 'lifeboat.com' not present\"\n",
      "\"Key 'citybeat.com' not present\"\n",
      "\"Key 'patreon.com' not present\"\n",
      "\"Key 'podbean.com' not present\"\n",
      "\"Key 'wwltv.com' not present\"\n",
      "\"Key '808ne.ws' not present\"\n",
      "\"Key 'sj-r.com' not present\"\n",
      "\"Key 'bbc.in' not present\"\n",
      "\"Key 'boards.net' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'lohud.com' not present\"\n",
      "\"Key 'yle.fi' not present\"\n",
      "\"Key 'statnews.com' not present\"\n",
      "\"Key 'eutimes.net' not present\"\n",
      "\"Key 'dearpandemic.org' not present\"\n",
      "\"Key 'thequint.com' not present\"\n",
      "\"Key 'wltx.com' not present\"\n",
      "\"Key 'lifeboat.com' not present\"\n",
      "\"Key 'healthgrades.com' not present\"\n",
      "\"Key 'ijidonline.com' not present\"\n",
      "\"Key 'theconversation.com' not present\"\n",
      "\"Key 'asianetnews.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'health.gov.au' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'nakedcapitalism.com' not present\"\n",
      "\"Key 'sciencemag.org' not present\"\n",
      "\"Key 'koin.com' not present\"\n",
      "\"Key 'medicalnewstoday.com' not present\"\n",
      "\"Key 'newschannel5.com' not present\"\n",
      "\"Key 'eutimes.net' not present\"\n",
      "\"Key 'odishatv.in' not present\"\n",
      "\"Key 'insider.com' not present\"\n",
      "\"Key 'whio.com' not present\"\n",
      "\"Key 'enca.com' not present\"\n",
      "\"Key 'sciencemag.org' not present\"\n",
      "\"Key 'practiceupdate.com' not present\"\n",
      "\"Key 'flip.it' not present\"\n",
      "\"Key 'anchor.fm' not present\"\n",
      "\"Key 'yle.fi' not present\"\n",
      "\"Key 'sciencemag.org' not present\"\n",
      "\"Key 'cleveland.com' not present\"\n",
      "\"Key 'cleveland.com' not present\"\n",
      "\"Key 'newstatesman.com' not present\"\n",
      "\"Key 'lcsun-news.com' not present\"\n",
      "\"Key 'news5cleveland.com' not present\"\n",
      "\"Key 'nbcchicago.com' not present\"\n",
      "\"Key 'mol.im' not present\"\n",
      "\"Key 'farmersweekly.co.za' not present\"\n",
      "\"Key 'usa366.com' not present\"\n",
      "\"Key 'economist.com' not present\"\n",
      "\"Key 'ivermectinforcovid.store' not present\"\n",
      "\"Key 'sciencemag.org' not present\"\n",
      "\"Key 'lifesitenews.com' not present\"\n",
      "\"Key 'huffpost.com' not present\"\n",
      "\"Key 'newsbreak.com' not present\"\n",
      "\"Key 'tribune.net.ph' not present\"\n",
      "\"Key 'newshub.lk' not present\"\n",
      "\"Key 'theedgemarkets.com' not present\"\n",
      "\"Key 'ksdk.com' not present\"\n",
      "\"Key 'al.com' not present\"\n",
      "\"Key 'health.gov.au' not present\"\n",
      "\"Key 'politicsweb.co.za' not present\"\n",
      "\"Key 'whio.com' not present\"\n",
      "\"Key 'shr.lc' not present\"\n",
      "\"Key 'saidit.net' not present\"\n",
      "\"Key 'cbsnews.com' not present\"\n",
      "\"Key 'flip.it' not present\"\n",
      "\"Key 'faqcheck.org' not present\"\n",
      "\"Key 'suntimes.com' not present\"\n",
      "\"Key 'odishatv.in' not present\"\n",
      "\"Key 'bbc.in' not present\"\n",
      "\"Key 'dearpandemic.org' not present\"\n",
      "\"Key 'nakedcapitalism.com' not present\"\n",
      "\"Key 'chwpeds.com' not present\"\n",
      "\"Key 'wirenews.org' not present\"\n",
      "\"Key 'wirenews.org' not present\"\n",
      "\"Key 'cleveland.com' not present\"\n",
      "\"Key 'marketwatch.com' not present\"\n",
      "\"Key 'cbs42.com' not present\"\n",
      "\"Key 'nbc6.com' not present\"\n",
      "\"Key 'suntimes.com' not present\"\n",
      "\"Key 'daily-sun.com' not present\"\n",
      "\"Key 'lcsun-news.com' not present\"\n",
      "\"Key 'themalaysianreserve.com' not present\"\n",
      "\"Key 'norwaytoday.info' not present\"\n",
      "\"Key 'king5.com' not present\"\n",
      "\"Key 'suntimes.com' not present\"\n",
      "\"Key 'lcsun-news.com' not present\"\n",
      "\"Key 'newschannel5.com' not present\"\n",
      "\"Key 'oregonstate.edu' not present\"\n",
      "\"Key 'odishatv.in' not present\"\n",
      "\"Key 'sciencedaily.com' not present\"\n",
      "\"Key 'wentworthreport.com' not present\"\n",
      "\"Key 'sciencedaily.com' not present\"\n",
      "\"Key 'news18.com' not present\"\n",
      "\"Key 'kxlh.com' not present\"\n",
      "\"Key 'wwltv.com' not present\"\n",
      "\"Key 'realityteam.org' not present\"\n",
      "\"Key 'wsws.org' not present\"\n",
      "\"Key 'clevelandclinic.org' not present\"\n",
      "\"Key 'farmersweekly.co.za' not present\"\n",
      "\"Key 'pharmacychecker.com' not present\"\n",
      "\"Key 'aapsonline.org' not present\"\n",
      "\"Key 'odishatv.in' not present\"\n",
      "\"Key 'gov.scot' not present\"\n",
      "\"Key 'lohud.com' not present\"\n",
      "\"Key 'gov.scot' not present\"\n",
      "\"Key 'timeslive.co.za' not present\"\n",
      "\"Key 'wirenews.org' not present\"\n",
      "\"Key 'nj.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'contagionlive.com' not present\"\n",
      "\"Key 'odishatv.in' not present\"\n",
      "\"Key 'medicalnewstoday.com' not present\"\n",
      "\"Key 'newstatesman.com' not present\"\n",
      "\"Key 'anchoragepress.com' not present\"\n",
      "\"Key 'iono.fm' not present\"\n",
      "\"Key 'blabber.buzz' not present\"\n",
      "\"Key 'cnbctv18.com' not present\"\n",
      "\"Key 'odishatv.in' not present\"\n",
      "\"Key 'ampproject.org' not present\"\n",
      "\"Key 'chwpeds.com' not present\"\n",
      "\"Key 'theedgemarkets.com' not present\"\n",
      "\"Key 'mdpi.com' not present\"\n",
      "\"Key 'entityart.co.uk' not present\"\n",
      "\"Key 'lohud.com' not present\"\n",
      "\"Key 'koin.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'bbc.in' not present\"\n",
      "\"Key 'health.gov.au' not present\"\n",
      "\"Key 'wkbw.com' not present\"\n",
      "\"Key 'wsws.org' not present\"\n",
      "\"Key 'ktvb.com' not present\"\n",
      "\"Key 'sciencedaily.com' not present\"\n",
      "\"Key 'suntimes.com' not present\"\n",
      "\"Key 'lcsun-news.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'huffpost.com' not present\"\n",
      "\"Key 'newschannel5.com' not present\"\n",
      "\"Key 'rokfin.com' not present\"\n",
      "\"Key 'flip.it' not present\"\n",
      "\"Key 'patreon.com' not present\"\n",
      "\"Key 'abcnews4.com' not present\"\n",
      "\"Key 'sci-hub.tw' not present\"\n",
      "\"Key 'sciencemag.org' not present\"\n",
      "\"Key 'as.com' not present\"\n",
      "\"Key 'sciencemag.org' not present\"\n",
      "\"Key 'citybeat.com' not present\"\n",
      "\"Key '2gtx.com' not present\"\n",
      "\"Key 'gizmodo.com' not present\"\n",
      "\"Key 'kansascity.com' not present\"\n",
      "\"Key 'lifeboat.com' not present\"\n",
      "\"Key 'lcsun-news.com' not present\"\n",
      "\"Key 'statnews.com' not present\"\n",
      "\"Key 'taiwannews.com.tw' not present\"\n",
      "\"Key 'theburningplatform.com' not present\"\n",
      "(20543, 302)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 1 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fbc18ef72732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0msmaller_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mNums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msmaller_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;31m#print(num_lst[Nums.index(num)],' is the smallest!')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0;31m# no multi-index, so validate all of the indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    836\u001b[0m                 \u001b[0;31m# We don't need to check for tuples here because those are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m                 \u001b[0;31m#  caught by the _is_nested_tuple_indexer check above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m                 \u001b[0;31m# We should never have a scalar section here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3774\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected label or tuple of labels, got {key}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3775\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3776\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3778\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    385\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "            import seaborn as sns\n",
    "            from sklearn.manifold import TSNE\n",
    "            import time\n",
    "            import matplotlib.pyplot as plt\n",
    "            %matplotlib inline\n",
    "\n",
    "            def export_tSNE(_data,save):\n",
    "                _array = _data.iloc[:,0:300].to_numpy()\n",
    "                time_start = time.time()\n",
    "                tsne = TSNE(n_components=2, verbose=1, random_state=42)\n",
    "                z = tsne.fit_transform(_array) \n",
    "                print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "                df = pd.DataFrame()\n",
    "                df[\"y\"] = _data.labels\n",
    "                df[\"comp-1\"] = z[:,0]\n",
    "                df[\"comp-2\"] = z[:,1]\n",
    "\n",
    "                sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),data=df).set(title=_target.capitalize()+\" tSNE projection\") \n",
    "                plt.savefig('../Project2/'+topic+'/figs/tSNE_'+_target+'_'+save+'_drop_duplicates.pdf')\n",
    "\n",
    "\n",
    "            smaller_sample=500\n",
    "            Nums=[smaller_sample, data.groupby('labels').size().reset_index().loc[0,0], data.groupby('labels').size().reset_index().loc[1,0]]\n",
    "            num = min(Nums)\n",
    "            #print(num_lst[Nums.index(num)],' is the smallest!')\n",
    "\n",
    "            #data.groupby('labels').size().reset_index().loc[0,0]\n",
    "\n",
    "            #data.groupby('labels').size().reset_index().loc[1,0]\n",
    "\n",
    "            ndf_c=data[data.labels==label1]\n",
    "\n",
    "            ndf_n=data[data.labels==label2]\n",
    "\n",
    "            ndf_c=ndf_c.sample(num)\n",
    "\n",
    "            ndf_n=ndf_n.sample(num)\n",
    "\n",
    "            df=pd.concat([ndf_c,ndf_n])\n",
    "            print('Processing finished!')\n",
    "\n",
    "            export_tSNE(df,str(num)+topic+ranking)\n",
    "\n",
    "            df.loc[df['labels']==label1, 'labels']=1\n",
    "\n",
    "            df.loc[df['labels']==label2, 'labels']=0\n",
    "\n",
    "            df['labels']=df['labels'].apply('int32')\n",
    "\n",
    "            def split_data(_df):\n",
    "                np.random.seed(42)\n",
    "                msk = np.random.rand(len(_df)) < 0.8\n",
    "                df_train = _df[msk]\n",
    "                df_left = _df[~msk]\n",
    "                np.random.seed(42)\n",
    "                msk = np.random.rand(len(df_left)) < 0.5\n",
    "                df_valid = df_left[msk]\n",
    "                df_test = df_left[~msk]\n",
    "\n",
    "                print(df.shape)\n",
    "                print('train:',df_train.shape)\n",
    "                print('validation',df_valid.shape)\n",
    "                print('test',df_test.shape)\n",
    "\n",
    "                # Get the lists of sentences and their labels.\n",
    "\n",
    "                training_data = torch.from_numpy(df_train.iloc[:,0:300].to_numpy())\n",
    "                validation_data = torch.from_numpy(df_valid.iloc[:,0:300].to_numpy())\n",
    "                testing_data = torch.from_numpy(df_test.iloc[:,0:300].to_numpy())\n",
    "\n",
    "                labels_train = torch.tensor(df_train['labels'].to_numpy())\n",
    "                labels_valid = torch.tensor(df_valid['labels'].to_numpy())\n",
    "                labels_test = torch.tensor(df_test['labels'].to_numpy()) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                _train_dataset = TensorDataset(training_data, labels_train)\n",
    "                _val_dataset =   TensorDataset(validation_data, labels_valid)\n",
    "                _test_dataset =  TensorDataset(testing_data, labels_test)\n",
    "\n",
    "\n",
    "                return _train_dataset, _val_dataset, _test_dataset #df_train, df_valid, df_test, labels_train, labels_valid, labels_test\n",
    "\n",
    "            train_dataset, val_dataset, test_dataset= split_data(df)\n",
    "\n",
    "            # Domain2Vec training\n",
    "\n",
    "            import wandb\n",
    "            class NetworkMLP(nn.Module):\n",
    "                def __init__(self):\n",
    "                    super(NetworkMLP, self).__init__() \n",
    "                    self.fc1 = nn.Linear(300, 150)\n",
    "\n",
    "                def forward(self, X):\n",
    "                    z1 = self.fc1(X)\n",
    "                    return z1 \n",
    "\n",
    "            class JointModel(nn.Module):\n",
    "\n",
    "                def __init__(self,hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "                    super().__init__()\n",
    "                    self.model_net = NetworkMLP()\n",
    "                    self.fc1 = nn.Linear(150*1, 600)\n",
    "                    self.fc2 = nn.Linear(600, output_dim) #original\n",
    "                    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "                def forward(self, _net): \n",
    "                    prediction= self.model_net(_net)\n",
    "\n",
    "                    #prediction_twtNews = self.model_twt(x_tNews)\n",
    "\n",
    "                    #concat_pred = torch.cat((prediction_RT, prediction_MT), 1)\n",
    "                    output = self.fc1(prediction)\n",
    "\n",
    "                    output = self.dropout(output) # add dropout\n",
    "                    output = self.fc2(F.relu(output)) #original\n",
    "                    #output = self.dropout(output)#add dropout\n",
    "                    #output = self.fc3(F.relu(output))\n",
    "                    return output\n",
    "\n",
    "            def count_parameters(model):\n",
    "                return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "            def binary_accuracy(preds, y):\n",
    "                \"\"\"\n",
    "                Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "                \"\"\"\n",
    "\n",
    "                #round predictions to the closest integer\n",
    "                #rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "                #rounded_preds = torch.round(preds)\n",
    "                correct = (preds == y).float() #convert into float for division \n",
    "                acc = correct.sum() / len(correct)\n",
    "\n",
    "                from sklearn.metrics import f1_score\n",
    "                macro_f1 = f1_score(y.to(\"cpu\"), preds.to(\"cpu\"), average='macro')\n",
    "                #print(\"macro_f1\", macro_f1)\n",
    "\n",
    "                return acc, macro_f1\n",
    "\n",
    "\n",
    "            def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "                epoch_loss = 0\n",
    "                epoch_acc = 0\n",
    "                epoch_macro = 0\n",
    "                model.train()\n",
    "                for step, batch in enumerate(iterator):    \n",
    "                #for batch in iterator:\n",
    "                    #print(\"batch\", batch)\n",
    "                    # `batch` contains three pytorch tensors:\n",
    "                    #   [0]: input ids \n",
    "                    #   [1]: attention masks\n",
    "                    #   [2]: labels \n",
    "                    #   b_input_ids = batch[0].to(device)\n",
    "                    #   b_input_mask = batch[1].to(device)\n",
    "                    #   b_labels = batch[2].to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    net = batch[0].to(device)\n",
    "\n",
    "                    label = batch[1].to(device)\n",
    "\n",
    "                    predictions = model(net)#,cosine_similarity)\n",
    "\n",
    "\n",
    "                    label = label.type(torch.LongTensor)\n",
    "\n",
    "\n",
    "                    loss = criterion(predictions, label)\n",
    "\n",
    "                    acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "                    epoch_acc += acc.item()\n",
    "                    epoch_macro += macro_f1.item()\n",
    "                return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)\n",
    "\n",
    "            def evaluate(model, iterator, criterion):\n",
    "                epoch_loss = 0\n",
    "                epoch_acc = 0\n",
    "                epoch_macro = 0\n",
    "                model.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    #for batch in iterator:\n",
    "                    for step, batch in enumerate(iterator):    \n",
    "\n",
    "                        net = batch[0].to(device)\n",
    "\n",
    "                        label = batch[1].to(device)\n",
    "\n",
    "                        label = label.type(torch.LongTensor)\n",
    "                        predictions = model(net)\n",
    "\n",
    "                        loss = criterion(predictions, label)\n",
    "\n",
    "                        acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "                        epoch_loss += loss.item()\n",
    "                        epoch_acc += acc.item()\n",
    "                        epoch_macro += macro_f1.item()\n",
    "                return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)\n",
    "\n",
    "\n",
    "            def epoch_time(start_time, end_time):\n",
    "                elapsed_time = end_time - start_time\n",
    "                elapsed_mins = int(elapsed_time / 60)\n",
    "                elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "                return elapsed_mins, elapsed_secs\n",
    "\n",
    "            def save_plots(train_losses, val_losses, train_f1, val_f1, train_accs, val_accs,test_accs,test_f1s):\n",
    "                \"\"\"Plot\n",
    "\n",
    "                    Plot two figures: loss vs. epoch and accuracy vs. epoch\n",
    "                \"\"\"\n",
    "                n = len(train_losses)\n",
    "                xs = np.arange(1,n+1,1)\n",
    "                xs = xs.astype(int)\n",
    "\n",
    "                # plot train and val losses\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.plot(xs, train_losses, '--', linewidth=2, label='train loss')\n",
    "                ax.plot(xs, val_losses, '-', linewidth=2, label='validation loss')\n",
    "                #ax.set_xlim(0, 10)\n",
    "                ax.set_xlabel(\"Epoch\")\n",
    "                ax.set_ylabel(\"Loss\")\n",
    "                ax.legend(loc='upper right')\n",
    "                plt.savefig('../Project2/'+topic+'/figs/'+savingname+'_Loss.png')\n",
    "\n",
    "                # plot train and val f1-score\n",
    "                #plt.clf()\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.plot(xs, train_f1, '--', linewidth=2, label='train')\n",
    "                ax.plot(xs, val_f1, '-', linewidth=2, label='validation')\n",
    "                ax.plot(xs, test_f1s, '-', linewidth=2, label='test')\n",
    "\n",
    "                ax.set_xlabel(\"Epoch\")\n",
    "                ax.set_ylabel(\"Macro-avg F1\")\n",
    "                ax.legend(loc='lower right')\n",
    "                plt.savefig('../Project2/'+topic+'/figs/'+savingname+'_Macro-avgF1.png')\n",
    "\n",
    "                # plot train and val accuracy\n",
    "                #plt.clf()\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.plot(xs, train_accs, '--', linewidth=2, label='train')\n",
    "                ax.plot(xs, val_accs, '-', linewidth=2, label='validation')\n",
    "                ax.plot(xs, test_accs, '-', linewidth=2, label='test')\n",
    "                #ax.set_xlim(0, 10)\n",
    "                ax.set_xlabel(\"Epoch\")\n",
    "                ax.set_ylabel(\"Accuracy\")\n",
    "                ax.legend(loc='lower right')\n",
    "                plt.savefig('../Project2/'+topic+'/figs/'+savingname+'_ACC.png')\n",
    "\n",
    "\n",
    "            savingname=topic+'_'+ranking+'_'+_target+'_:'+str(num)+'_drop_duplicates'\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            print('Training')\n",
    "            print('Device:',device)\n",
    "            which_time = 0\n",
    "            N_EPOCHS = 10\n",
    "            HIDDEN_DIM = 300 \n",
    "            OUTPUT_DIM = 3\n",
    "            N_LAYERS = 2\n",
    "            BIDIRECTIONAL = True\n",
    "            DROPOUT = 0.5 \n",
    "\n",
    "            train_dataloader = DataLoader(\n",
    "                                train_dataset,  # The training samples.\n",
    "                                sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "                                batch_size = batch_size # Trains with this batch size.\n",
    "                            )\n",
    "            validation_dataloader = DataLoader(\n",
    "                        val_dataset, # The validation samples.\n",
    "                        sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "                        batch_size = batch_size # Evaluate with this batch size.\n",
    "                    )\n",
    "            test_dataloader = DataLoader(\n",
    "                        test_dataset, # The validation samples.\n",
    "                        sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "                        batch_size = batch_size # Evaluate with this batch size.\n",
    "                    ) \n",
    "\n",
    "            model = JointModel(HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL,DROPOUT)\n",
    "            optimizer = optim.Adam(model.parameters())\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            criterion = criterion.to(device) \n",
    "            wandb.login()\n",
    "            wandb.init(project='Project2_joint',name=savingname)#wandb.init(project='Project2_joint',name=savingname+'_'+str(eachtime))\n",
    "            wandb.config = {\n",
    "                    \"epochs\": N_EPOCHS,\n",
    "                    \"batch_size\": batch_size\n",
    "                    }\n",
    "\n",
    "            resume_training = False\n",
    "            if resume_training == True:\n",
    "                eachtime = 0\n",
    "                which_time = 6\n",
    "                path= '../Project2/pt/'+savingname+'_'+str(eachtime)+'/'\n",
    "                checkpoint = torch.load(path+str(which_time)+'.pt')\n",
    "                model.load_state_dict(checkpoint)\n",
    "            model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pt=makefolder('../Project2/pt/'+savingname+'/')#pt=makefolder('../Project2/pt/'+savingname+'_'+str(eachtime)+'/')\n",
    "            pt_best=makefolder('../Project2/pt_best/'+savingname+'/')#pt_best=makefolder('../Project2/pt_best/'+savingname+'_'+str(eachtime)+'/')\n",
    "\n",
    "\n",
    "            best_valid_loss = float('inf')\n",
    "            per_epoch_train_loss = []\n",
    "            per_epoch_val_loss = []\n",
    "            per_epoch_train_f1 = []\n",
    "            per_epoch_val_f1 = []\n",
    "            per_epoch_train_acc = []\n",
    "            per_epoch_val_acc = []\n",
    "\n",
    "            import time\n",
    "            t1=time.time()\n",
    "            for epoch in range(which_time, N_EPOCHS):\n",
    "                print(epoch)\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                train_loss, train_acc, train_f1 = train(model, train_dataloader, optimizer, criterion)\n",
    "                valid_loss, valid_acc, valid_f1 = evaluate(model, validation_dataloader, criterion)\n",
    "\n",
    "                end_time = time.time()\n",
    "\n",
    "                epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "                per_epoch_train_loss.append(train_loss)\n",
    "                per_epoch_val_loss.append(valid_loss)\n",
    "                per_epoch_train_f1.append(train_f1)\n",
    "                per_epoch_val_f1.append(valid_f1)\n",
    "                per_epoch_train_acc.append(train_acc)\n",
    "                per_epoch_val_acc.append(valid_acc)\n",
    "                Val_aver_acc=mean(per_epoch_val_acc)\n",
    "                Val_aver_f1=mean(per_epoch_val_f1)\n",
    "\n",
    "\n",
    "                if valid_loss <= best_valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    print(\"best model saved in epoch :\", epoch+1 )\n",
    "                    torch.save(model.state_dict(), pt_best +str(epoch+1)+'.pt')\n",
    "                torch.save(model.state_dict(), pt +'/'+str(epoch+1)+'.pt')\n",
    "\n",
    "                print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "                print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} | Train macro-avg-f1: {train_f1*100:.2f}%')\n",
    "                print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f} |  Val. macro-avg-f1: {valid_f1*100:.2f}%')\n",
    "                print(f'\\t Aver. ACC: {Val_aver_acc*100:.2f}%')\n",
    "                wandb.log({'Max Val Acc':max(per_epoch_val_acc),'Average Val. Acc': Val_aver_acc,'Val. accuracy': valid_acc, 'Val. loss': valid_loss,'Val. macro-avg-f1':valid_f1})\n",
    "\n",
    "\n",
    "            Test_Acc = []\n",
    "            Test_f1=[]\n",
    "            for epoch in range(N_EPOCHS):\n",
    "                model.load_state_dict(torch.load(pt+str(epoch+1)+'.pt')) \n",
    "                test_loss, test_acc, test_f1 = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "                Test_Acc.append(test_acc)\n",
    "                Test_f1.append(test_f1)\n",
    "                Test_aver_acc=mean(Test_Acc)\n",
    "                Test_test_f1=mean(Test_f1)\n",
    "                t2=time.time()\n",
    "                print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f} | Test macro-avg-f1: {test_f1*100:.2f}%')\n",
    "                print(f'\\t Aver. TEST ACC: {Test_aver_acc*100:.2f}%')\n",
    "                wandb.log({'Max TEST Acc':max(Test_Acc),'Average TEST Acc': Test_aver_acc,'Test accuracy': test_acc, 'Test loss': test_loss,'Test macro-avg-f1':Test_test_f1})\n",
    "            _time=t2-t1\n",
    "            wandb.finish()\n",
    "            save_plots(per_epoch_train_loss, per_epoch_val_loss, per_epoch_train_f1, \n",
    "            per_epoch_val_f1, per_epoch_train_acc, per_epoch_val_acc,Test_Acc,Test_f1)\n",
    "            re=pd.DataFrame([[topic,_target,num*2,Val_aver_acc,Test_aver_acc, Val_aver_f1, Test_test_f1,_time]],columns=['Topic','NEWS/TEXT','samples','Avg. Val. Acc.','Avg. Test.Acc','Avg. Val. macro-avg-f1','Avg. Test. macro-avg-f1','Time'])\n",
    "            re.to_csv('../Project2/Ivermectin&Biden_DOMAIN&HASHTAG_classification_results_drop_duplicates.csv',index=None,mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>NEWS/TEXT</th>\n",
       "      <th>samples</th>\n",
       "      <th>Avg. Val. Acc.</th>\n",
       "      <th>Avg. Test.Acc</th>\n",
       "      <th>Avg. Val. macro-avg-f1</th>\n",
       "      <th>Avg. Test. macro-avg-f1</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>biden</td>\n",
       "      <td>domain</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.856548</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.599737</td>\n",
       "      <td>0.741584</td>\n",
       "      <td>2.274116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic NEWS/TEXT  samples  Avg. Val. Acc.  Avg. Test.Acc  \\\n",
       "0  biden    domain     1000        0.856548       0.942857   \n",
       "\n",
       "   Avg. Val. macro-avg-f1  Avg. Test. macro-avg-f1      Time  \n",
       "0                0.599737                 0.741584  2.274116  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004nino</td>\n",
       "      <td>[0.00067769457, -0.0046113823, 4.315062e-05, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0057005</td>\n",
       "      <td>[0.0030170067, 1.2023449e-05, -0.0016398143, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>007Battledress</td>\n",
       "      <td>[0.0010162839, -0.00076382636, 0.0024674176, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>007Haarp</td>\n",
       "      <td>[0.0010162839, -0.00076382636, 0.0024674176, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010Bravo</td>\n",
       "      <td>[0.00010244211, -0.0005833117, -0.0007128835, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20538</th>\n",
       "      <td>zwarteaap</td>\n",
       "      <td>[-0.00028196812, -0.00015717825, 0.002299595, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20539</th>\n",
       "      <td>zxcvbn7531</td>\n",
       "      <td>[0.0032451407, 0.0013824304, -0.00064874016, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20540</th>\n",
       "      <td>zxcvbnlkjhgf</td>\n",
       "      <td>[0.0005242489, 0.0042879223, 0.0007735149, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20541</th>\n",
       "      <td>zywiecPolska</td>\n",
       "      <td>[0.0013181877, -0.002224799, -0.0007176336, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20542</th>\n",
       "      <td>zzcbw</td>\n",
       "      <td>[0.00023692766, 0.0026868344, -0.0015930708, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20543 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_screen_name                                                vec\n",
       "0              004nino  [0.00067769457, -0.0046113823, 4.315062e-05, -...\n",
       "1              0057005  [0.0030170067, 1.2023449e-05, -0.0016398143, -...\n",
       "2       007Battledress  [0.0010162839, -0.00076382636, 0.0024674176, -...\n",
       "3             007Haarp  [0.0010162839, -0.00076382636, 0.0024674176, -...\n",
       "4             010Bravo  [0.00010244211, -0.0005833117, -0.0007128835, ...\n",
       "...                ...                                                ...\n",
       "20538        zwarteaap  [-0.00028196812, -0.00015717825, 0.002299595, ...\n",
       "20539       zxcvbn7531  [0.0032451407, 0.0013824304, -0.00064874016, -...\n",
       "20540     zxcvbnlkjhgf  [0.0005242489, 0.0042879223, 0.0007735149, -0....\n",
       "20541     zywiecPolska  [0.0013181877, -0.002224799, -0.0007176336, -0...\n",
       "20542            zzcbw  [0.00023692766, 0.0026868344, -0.0015930708, -...\n",
       "\n",
       "[20543 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Misinformation-related</td>\n",
       "      <td>1418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   labels     0\n",
       "0  Misinformation-related  1418"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('labels').size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check domain label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_co_occur_df=pd.read_csv('../Project2/biden/biden_co_occur.csv')\n",
    "nodes1=pd.read_csv('../Project2/domain/data.csv')['Domain'].tolist()\n",
    "nodes2=pd.read_csv('../Project2/domain/cre.csv')['domain'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_co_occur_df.loc[biden_co_occur_df['Id'].isin(nodes1),'flag']='non-credible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_co_occur_df.loc[biden_co_occur_df['Id'].isin(nodes2),'flag']='credible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_co_occur_df[biden_co_occur_df['modularity_class']==0].groupby('flag').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(208+1)/(208-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "biden_co_occur_df[biden_co_occur_df['modularity_class']==0].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_co_occur_df[biden_co_occur_df['modularity_class']==1].groupby('flag').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "biden_co_occur_df[biden_co_occur_df['modularity_class']==1].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(64+46)/(64-46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "dfn1=pd.DataFrame(nodes1)\n",
    "dfn1.columns=['user_screen_name']\n",
    "dfn1['label']='pro-QAnon'\n",
    "\n",
    "dfn2=pd.DataFrame(nodes2)\n",
    "dfn2.columns=['user_screen_name']\n",
    "dfn2['label']='anti-QAnon'\n",
    "\n",
    "dfn=pd.concat([dfn1,dfn2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn.to_csv('../Project2/domain/user_label.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=nx.read_gexf('../Project2/domain/domain.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf=nx.to_pandas_edgelist(K,'source','target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes1=pd.read_csv('../Project2/domain/data.csv')['Domain'].tolist()\n",
    "nodes2=pd.read_csv('../Project2/domain/cre.csv')['domain'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.loc[kdf['source'].isin(nodes1),'Flag']='pro-QAnon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.loc[kdf['source'].isin(nodes2),'Flag']='anti-QAnon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.to_csv('../Project2/domain/edge_list.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G,'../Project2/domains.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_K=nx.from_pandas_edgelist(_hadf,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf0['urls'].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy goodbots\n",
    "n1=df2['user_screen_name'].shape[0]\n",
    "temp = mdf[mdf['class'] == 'center users']\n",
    "n2=temp[temp['users'].isin(df2['user_screen_name'])].shape[0]\n",
    "n2/n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speedtest\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "import logging\n",
    "try:\n",
    "    s = speedtest.Speedtest()\n",
    "\n",
    "    with open('test.csv', mode='w') as speedcsv:\n",
    "        csv_writer = csv.DictWriter(speedcsv, fieldnames=['time', 'downspeed', 'upspeed'])\n",
    "        csv_writer.writeheader()\n",
    "        while True:\n",
    "            time_now = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "            downspeed = round((round(s.download()) / 1048576), 2)\n",
    "            upspeed = round((round(s.upload()) / 1048576), 2)\n",
    "            csv_writer.writerow({\n",
    "                'time': time_now,\n",
    "                'downspeed': downspeed,\n",
    "                \"upspeed\": upspeed\n",
    "            })\n",
    "            # 60 seconds sleep\n",
    "            time.sleep(60)\n",
    "except Exception as e:\n",
    "            logging.error('Exception raised.')\n",
    "            logging.error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='http://google&p'\n",
    "\n",
    "'google' & 'amp' in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find news 长度分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1, 2, 3' \n",
    "#....to install apex...\n",
    "#pip install -v --no-cache-dir ./\n",
    "\n",
    "def extract_domain(x):\n",
    "    '''\n",
    "    e.g. x='http://www.bbc.co.uk'\n",
    "    '''\n",
    "    ext = tldextract.extract(x)\n",
    "    return ext.registered_domain\n",
    "\n",
    "def retrieveRTMT(_df,n2vRT,n2vMT):\n",
    "    '''\n",
    "    #_names: nodes' name in RT or MT network\n",
    "    '''\n",
    "    \n",
    "    dfRT=pd.DataFrame(n2vRT.wv.vectors,index=n2vRT.wv.index2entity)\n",
    "    dfMT=pd.DataFrame(n2vMT.wv.vectors,index=n2vMT.wv.index2entity)\n",
    "    RT=dfRT[dfRT.index.isin(_df['user_screen_name'])].values\n",
    "    MT=dfMT[dfMT.index.isin(_df['user_screen_name'])].values\n",
    "    \n",
    "    \n",
    "    RT=torch.tensor(RT)\n",
    "    MT=torch.tensor(MT)\n",
    "    return RT, MT\n",
    "\n",
    "def prepro(ndf,col):\n",
    "    \n",
    "    import re\n",
    "    def remove_rt(query):\n",
    "        query=re.sub(r\"\\brt\", \"\", query)\n",
    "        return query\n",
    "    ndf[col]=ndf[col].progress_apply(lambda x: remove_rt(x))\n",
    "\n",
    "    def tokenizer(_text):\n",
    "        return TweetTokenizer().tokenize(_text)\n",
    "\n",
    "    ndf[col]=ndf[col].progress_apply(lambda x:tokenizer(x))\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    STOPWORDS=stopwords.words('english')\n",
    "    def remove_stop(_text):\n",
    "        return ' '.join([x for x in _text if x not in STOPWORDS])\n",
    "\n",
    "    ndf[col]=ndf[col].progress_apply(lambda x: remove_stop(x))\n",
    "    \n",
    "    def preprocessing(string): \n",
    "\n",
    "        text=clean(string,lower=True, no_emails=True,no_numbers=True,no_punct=True,no_digits=False,no_currency_symbols=True,\n",
    "                   replace_with_number=\"\",\n",
    "                   no_urls=True,replace_with_url=\"\",\n",
    "                   replace_with_email=\"\",\n",
    "                   replace_with_currency_symbol=\"\")\n",
    "\n",
    "        return text\n",
    "    ndf[col]=ndf[col].progress_apply(lambda x: preprocessing(x))\n",
    "    \n",
    "    def remove_emoji(string):\n",
    "        p.set_options(p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "        \n",
    "        return p.clean(string)\n",
    "    ndf[col]=ndf[col].progress_apply(lambda x: remove_emoji(x))\n",
    "    \n",
    "    ndf.reset_index(inplace=True,drop=True)\n",
    "    return ndf\n",
    "\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.enabled = False\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "def makefolder(save):\n",
    "    if  os.path.exists(save) == False:\n",
    "        os.makedirs(save)\n",
    "    else:\n",
    "        print('Floder:',save,'Exsit!')\n",
    "    return save\n",
    "def get_intersection(_df,_intersecion):\n",
    "    for each  in ['source','target']:\n",
    "\n",
    "        _df=_df[_df[each].isin(_intersecion)]\n",
    "    return _df\n",
    "\n",
    "def fast_node2vec(_df,_save,_file,saving_name,epochs):\n",
    "    '''\n",
    "    _df:retweetdf,mentiondf\n",
    "    _save:saving folder path\n",
    "    _file:csv name, e.g., vaccine, mask\n",
    "    saving_name: RT or MT\n",
    "    '''\n",
    "    \n",
    "    _tuples=_df.to_records(index=False)\n",
    "\n",
    "    _lst=list(_tuples)\n",
    "\n",
    "    _graph = Graph(_lst, directed=True, weighted=True)\n",
    "\n",
    "    _n2v = Node2Vec(_graph, dim=300, walk_length=100, context=10, p=2.0, q=0.5, workers=-1)\n",
    "    \n",
    "    _n2v.train(epochs=epochs)\n",
    "\n",
    "    _n2v.wv.save(_save+_file+'_'+saving_name+'.wv')\n",
    "    return _n2v\n",
    "\n",
    "def updatecol(df1,df2,colname,index):\n",
    "    df1[colname]=''\n",
    "\n",
    "    df1.set_index(index,inplace=True)\n",
    "\n",
    "    df2.set_index(index,inplace=True)\n",
    "\n",
    "    df1.update(df2)\n",
    "\n",
    "    df1.reset_index(inplace=True)\n",
    "    df2.reset_index(inplace=True)\n",
    "    return df1\n",
    "\n",
    "def updatecol_seperate(df1,df2,colname,index1,index2):\n",
    "    df1[colname]=''\n",
    "\n",
    "    df1.set_index(index1,inplace=True)\n",
    "\n",
    "    df2.set_index(index2,inplace=True)\n",
    "\n",
    "    df1.update(df2)\n",
    "\n",
    "    df1.reset_index(inplace=True)\n",
    "    df2.reset_index(inplace=True)\n",
    "    return df1\n",
    "\n",
    "\n",
    "\n",
    "def preproNetwork(_df,nettype,topn,Need,_name):\n",
    "    '''\n",
    "    target: 'retweeted_status_screen_name' or 'user_mentions_screen_names'\n",
    "    return topn indegree dataframe\n",
    "    _name: QAnon or Biden\n",
    "    '''\n",
    "    if nettype=='RT':\n",
    "        target= 'retweeted_status_screen_name' \n",
    "    elif nettype=='MT':\n",
    "        target= 'user_mentions_screen_names'\n",
    "        \n",
    "    file=_name\n",
    "    save='../Project2/health/'+file+'_data_processed/'\n",
    "    makefolder(save)\n",
    "        \n",
    "    #_df=_df.rename(columns={'user_screen_name':'users'})\n",
    "    _df=_df[['user_screen_name',target]]\n",
    "    _df.dropna(subset=[target],inplace=True)\n",
    "    _df[target]=_df[target].str.replace('\\'','')\n",
    "    _df[target]=_df[target].apply(lambda x: x.split(','))\n",
    "    _df=_df.explode([target])\n",
    "    _df=_df[['user_screen_name',target]]\n",
    "    #_df.columns=['source','target']\n",
    "    \n",
    "    \n",
    "    _df=_df.explode([target])\n",
    "    \n",
    "    G=nx.from_pandas_edgelist(_df,source='user_screen_name',target=target,create_using=nx.DiGraph())\n",
    "\n",
    "    nx.write_gexf(G, save+nettype+ '.gexf')\n",
    "    mdf=pd.DataFrame(G.in_degree)\n",
    "    mdf.columns=['Allusers','indegree']\n",
    "    kdf=mdf.sort_values('indegree',ascending=False).head(topn)\n",
    "    kdf.to_csv('../Project2/health/top'+str(topn)+nettype+'indegree.csv',index=None)\n",
    "    #print(_df.columns)\n",
    "    #print(kdf.columns)\n",
    "    ndf=updatecol_seperate(_df,kdf,'indegree','user_screen_name','Allusers')\n",
    "    ldf=ndf[ndf['indegree']!='']\n",
    "    ldf['indegree']=ldf['indegree'].astype(int)\n",
    "    ldf=ldf.sort_values('indegree',ascending=False)\n",
    "    \n",
    "    #produce node2vec\n",
    "    if Need==True:\n",
    "        __df=_df.groupby(['user_screen_name',target]).size().reset_index()\n",
    "        __df=__df.rename(columns={0:'weight'})\n",
    "        n2v=fast_node2vec(__df, save, file+'_'+nettype+'_weighted',nettype,500)\n",
    "    else:\n",
    "        n2v=None\n",
    "    return ldf, n2v\n",
    "\n",
    "def rank_indegree(_df1,_indegree,usertypes,rank):\n",
    "    _df1p=updatecol_seperate(_df1,_indegree,'indegree',usertypes[rank],'user_screen_name')\n",
    "    _df1p['indegree']=pd.to_numeric(_df1p['indegree'])\n",
    "    #df1p['indegree']=df1p['indegree'].astype(int)\n",
    "    _df1p=_df1p.sort_values('indegree',ascending=False).drop_duplicates(usertypes[rank])\n",
    "    #print(df1p.shape)\n",
    "    return _df1p\n",
    "\n",
    "\n",
    "def get_textdf(_df):\n",
    "    #_df=_df[['user_screen_name','text','news']]\n",
    "    #_df=_df.groupby(by='user_screen_name').agg(text=(\"text\", lambda x: \",\".join(set(x))))\n",
    "    #_df.reset_index(inplace=True)\n",
    "    \n",
    "    _t1=_df.groupby(by='user_screen_name').agg(text=(\"text\", lambda x: \",\".join(set(x))))\n",
    "\n",
    "    _t2=_df.groupby(by='user_screen_name').agg(news=(\"news\", lambda x: \",\".join(set(x))))\n",
    "    \n",
    "    _t1['news']=''\n",
    "    \n",
    "    _t1.update(_t2)\n",
    "    \n",
    "    _t1.reset_index(inplace=True)\n",
    "    return _t1\n",
    "\n",
    "class cos_feature:\n",
    "    def __init__(self, text, news):\n",
    "        self.long_roberta = long_roberta\n",
    "        self.text = text\n",
    "        self.news = news\n",
    "\n",
    "    def cos_similarity(self):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = self.long_roberta(self.text)[0]\n",
    "        return embedding\n",
    "\n",
    "    def emb_similarity(self):\n",
    "\n",
    "        emb1 = self.cos_similarity(self.text)\n",
    "        emb2 = self.cos_similarity(self.news)\n",
    "        emb1 = emb1.cpu().detach().numpy()\n",
    "        emb2  = emb2.cpu().detach().numpy()\n",
    "        print('tweet:',emb1.shape)\n",
    "        print('news:', emb2.shape)\n",
    "        Y = cdist(emb1[0], emb2[0], 'cosine')[0][0]\n",
    "        cos_sim = 1.0 - Y\n",
    "        print('similarity:',cos_sim)\n",
    "        return cos_sim\n",
    "\n",
    "class ROBERTALSTMSentiment(nn.Module):\n",
    "        def __init__(self,\n",
    "                    long_roberta,\n",
    "                    hidden_dim,\n",
    "                    output_dim,\n",
    "                    n_layers,\n",
    "                    bidirectional,\n",
    "                    dropout):\n",
    "            \n",
    "            super().__init__()\n",
    "            \n",
    "            self.long_roberta = long_roberta\n",
    "            \n",
    "            embedding_dim = long_roberta.config.to_dict()['hidden_size'] #768\n",
    "                \n",
    "            self.rnn = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers = n_layers,\n",
    "                            bidirectional = bidirectional,\n",
    "                            batch_first = True,\n",
    "                            dropout = 0 if n_layers < 2 else dropout)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.attn_fc = Attention(hidden_dim * 2 if bidirectional else hidden_dim) #attention layer from torchnlp\n",
    "            self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "        def forward(self, text):\n",
    "            with torch.no_grad():\n",
    "                embedded = self.long_roberta(text)[0]\n",
    "            lstm_out, (hidden, c_n) = self.rnn(embedded)\n",
    "\n",
    "            if self.rnn.bidirectional: #add dropout\n",
    "                hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "            else:\n",
    "                hidden = self.dropout(hidden[-1,:,:])\n",
    "  \n",
    "            attn_out = self.attn_fc(hidden.unsqueeze(1), lstm_out)\n",
    "           \n",
    "            return attn_out[0].squeeze(1)\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "def token_id_twt(sentences):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        # For every sentence...\n",
    "        for sent in sentences:\n",
    "            # `encode_plus` will:\n",
    "            #   (1) Tokenize the sentence.\n",
    "            #   (2) Prepend the `[CLS]` token to the start.\n",
    "            #   (3) Append the `[SEP]` token to the end.\n",
    "            #   (4) Map tokens to their IDs.\n",
    "            #   (5) Pad or truncate the sentence to `max_length`\n",
    "            #   (6) Create attention masks for [PAD] tokens.\n",
    "            encoded_dict = longformer_tokenizer.encode_plus(\n",
    "                                sent,                      # Sentence to encode.\n",
    "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                max_length = 4000,           # Pad & truncate all sentences.\n",
    "                                pad_to_max_length = True,\n",
    "                                return_attention_mask = True,   # Construct attn. masks.\n",
    "                                truncation=True, #explicitely truncate examples to max length. #my add\n",
    "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        )\n",
    "\n",
    "            # Add the encoded sentence to the list.    \n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            #print(type(input_ids[0]), input_ids[0], input_ids[0].size())\n",
    "            #sys.exit()\n",
    "\n",
    "            # And its attention mask (simply differentiates padding from non-padding).\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        # Convert the lists into tensors.\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "        #labels = torch.tensor(labels)\n",
    "\n",
    "    #     # Print sentence 0, now as a list of IDs.\n",
    "    #     print('Original: ', sentences_train[0])\n",
    "    #     print('Token IDs:', input_ids_train[0])\n",
    "        return input_ids, attention_masks\n",
    "\n",
    "class NetworkMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkMLP, self).__init__() \n",
    "        self.fc1 = nn.Linear(300, 150)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        z1 = self.fc1(X)\n",
    "        return z1 \n",
    "\n",
    "class JointModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.model_net = NetworkMLP()\n",
    "        self.model_twt = ROBERTALSTMSentiment(long_roberta, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "        self.fc1 = nn.Linear(600, 600)\n",
    "        self.fc2 = nn.Linear(600, output_dim) #original\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x_t): \n",
    "        prediction_twt = self.model_twt(x_t)\n",
    "        #prediction_twtNews = self.model_twt(x_tNews)\n",
    "       \n",
    "        #concat_pred = torch.cat((prediction_twt, prediction_twtNews), 1)\n",
    "        output = self.fc1(prediction_twt)\n",
    "        \n",
    "        output = self.dropout(output) # add dropout\n",
    "        output = self.fc2(F.relu(output)) #original\n",
    "        #output = self.dropout(output)#add dropout\n",
    "        #output = self.fc3(F.relu(output))\n",
    "        return output\n",
    "        \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    #rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    #rounded_preds = torch.round(preds)\n",
    "    correct = (preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    \n",
    "    from sklearn.metrics import f1_score\n",
    "    macro_f1 = f1_score(y.to(\"cpu\"), preds.to(\"cpu\"), average='macro')\n",
    "    #print(\"macro_f1\", macro_f1)\n",
    "\n",
    "    return acc, macro_f1\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(iterator):    \n",
    "    #for batch in iterator:\n",
    "        #print(\"batch\", batch)\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        #   b_input_ids = batch[0].to(device)\n",
    "        #   b_input_mask = batch[1].to(device)\n",
    "        #   b_labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #netRT = batch[0].to(device)\n",
    "        #netMT = batch[1].to(device)\n",
    "        #feature1=batch[0]\n",
    "        #print('feature1:',feature1)\n",
    "        #feature2=batch[1]\n",
    "        #cosine_score=emb_similarity(feature1, feature2)\n",
    "        #cosine_score_view=cosine_score.view(cosine_score.size()[0],1)\n",
    "\n",
    "        twt = batch[0].to(device)\n",
    "        #news= batch[1].to(device)\n",
    "\n",
    "        #print('text.size:',twt.size())\n",
    "        #print('news.size:',news.size())\n",
    "\n",
    "        #cosine_similarity=cosine_similarity.to(device)\n",
    "        label = batch[1].to(device)\n",
    "\n",
    "        #print(\"label\", label, type(label),label.size()) #torch.Size([32])\n",
    "        #label = label.unsqueeze(1)\n",
    "        #print(\"label\", label, type(label),label.size())\n",
    "        #predictions = model(batch.text).squeeze(1)\n",
    "        #predictions = model(text)\n",
    "        predictions = model(twt)#,cosine_similarity)\n",
    "\n",
    "        #cosine_score_view=cosine_score.view(cosine_score.size()[0],1)\n",
    "\n",
    "\n",
    "\n",
    "        #print('cosine_similarity.size:',cosine_score.size())\n",
    "        #predictions=torch.cat((predictions,cosine_score_view), dim = 1)\n",
    "        #print('predictions.size:',predictions.size())\n",
    "        loss = criterion(predictions, label)\n",
    "    \n",
    "        acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_macro += macro_f1.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_macro = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        #for batch in iterator:\n",
    "        for step, batch in enumerate(iterator):    \n",
    "            #des = batch[0].to(device)\n",
    "            #loc = batch[1].to(device)\n",
    "            #netRT = batch[0].to(device)\n",
    "            #netMT = batch[1].to(device)\n",
    "            twt = batch[0].to(device)\n",
    "            #news= batch[1].to(devifce)\n",
    "            label = batch[1].to(device)\n",
    "            #predictions = model(batch.text).squeeze(1)\n",
    "            predictions = model(twt)\n",
    "            #loss = criterion(predictions, batch.label)\n",
    "            loss = criterion(predictions, label)\n",
    "            #acc = binary_accuracy(predictions, batch.label)\n",
    "            acc, macro_f1 = binary_accuracy(torch.argmax(predictions, dim = 1), label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_macro += macro_f1.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_macro / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    path = '../Project2/health/3in1/'\n",
    "    file_name = 'Ivermectin'\n",
    "    \n",
    "    \n",
    "    name1 , name2 = 'badbots','goodbots'\n",
    "    #if (name1 != 'badbots') | (name2 != 'badhumans'):\n",
    "    print('name1:',name1)\n",
    "    print('name2:',name2)\n",
    "    #name1='badbots'\n",
    "    #name2='badhumans'#'badhumans'#'goodbots'\n",
    "    smaller_sample = 500\n",
    "    batch_size = 16\n",
    "    #savingname='degreeRank'+name1+'VS'+name2+'_Health_TEXT_weighted_directed_batch'+str(batch_size)+'violinIndex'+'TokenLength>30'+'Token=4000'\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Training')\n",
    "    print('Device:',device)\n",
    "    which_time = 0\n",
    "    N_EPOCHS = 10\n",
    "    HIDDEN_DIM = 300 \n",
    "    OUTPUT_DIM = 3\n",
    "    N_LAYERS = 2\n",
    "    BIDIRECTIONAL = True\n",
    "    DROPOUT = 0.5 \n",
    "\n",
    "    \n",
    "    #news=pd.read_csv(path+file_name+'urlsNews.csv')\n",
    "    #tem=pd.read_csv('../Project3/gf.csv')\n",
    "    ndf=gf=df=pd.read_csv(path+'health_selectedIvermectin.csv')\n",
    "    modularity=pd.read_csv(path+file_name+'_networkx.csv')\n",
    "  \n",
    "    usertypes=['user_screen_name','retweeted_status_screen_name','user_mentions_screen_names']\n",
    "    rank=0\n",
    "    topn=1000000\n",
    "    \n",
    "\n",
    "    \n",
    "    modularity=modularity.rename(columns={'Id':'user_screen_name'})\n",
    "\n",
    "    ndf=updatecol(ndf,modularity,'modularity_class','user_screen_name')\n",
    "    \n",
    "    if topic == 'biden':\n",
    "        m2=52\n",
    "    elif topic == 'Ivermectin':\n",
    "        m2=241\n",
    "\n",
    "    nodes1=ndf[ndf['modularity_class']==38]['user_screen_name'].drop_duplicates().tolist()# Iverlectin related\n",
    "    nodes2=ndf[ndf['modularity_class']==m2]['user_screen_name'].drop_duplicates().tolist()# mainstream related\n",
    "\n",
    "    df1=ndf[ndf['user_screen_name'].isin(nodes1)]\n",
    "    df2=ndf[ndf['user_screen_name'].isin(nodes2)]\n",
    "\n",
    "    print(name1+' with democratic users:',len(set(df1[usertypes[rank]])))\n",
    "    print(name2+' with republican users:',len(set(df2[usertypes[rank]])))\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['news'].str.split().str.len().plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['text'].str.split().str.len().plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
